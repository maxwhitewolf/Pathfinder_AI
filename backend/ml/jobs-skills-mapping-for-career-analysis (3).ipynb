{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PathFinder AI - Phase 2: Doc2Vec Job Matcher\n",
    "Dataset: Jobs & Skills Mapping for Career Analysis\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.utils import simple_preprocess\n",
    "import joblib\n",
    "import json\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load Dataset 2\n",
    "print(\"=\"*70)\n",
    "print(\"PHASE 2: DOC2VEC JOB MATCHER\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Update this path to your dataset location\n",
    "df = pd.read_csv('/kaggle/input/jobs-and-skills-mapping-for-career-analysis/formatted_jobs.csv')\n",
    "\n",
    "print(f\"\\nDataset loaded: {len(df)} records\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(\"\\nFirst 3 rows:\")\n",
    "print(df.head(3))\n",
    "\n",
    "# Data Preprocessing\n",
    "print(\"\\n--- Preprocessing Job Data ---\")\n",
    "\n",
    "# Create job documents by combining all text fields\n",
    "def create_job_document(row):\n",
    "    \"\"\"Combine job title, description, and skills into one document\"\"\"\n",
    "    title = str(row['job_title'])\n",
    "    description = str(row['Short_description'])\n",
    "    skills = str(row['Skills_required'])\n",
    "    \n",
    "    # Combine all fields\n",
    "    document = f\"{title} {description} {skills}\"\n",
    "    return document\n",
    "\n",
    "df['job_document'] = df.apply(create_job_document, axis=1)\n",
    "\n",
    "print(f\"Sample job document:\\n{df['job_document'].iloc[0][:200]}...\")\n",
    "\n",
    "# Tokenize documents\n",
    "print(\"\\n--- Tokenizing Documents ---\")\n",
    "\n",
    "def tokenize_document(doc):\n",
    "    \"\"\"Simple tokenization and cleaning\"\"\"\n",
    "    return simple_preprocess(doc, deacc=True, min_len=2, max_len=15)\n",
    "\n",
    "df['tokens'] = df['job_document'].apply(tokenize_document)\n",
    "\n",
    "print(f\"Sample tokens: {df['tokens'].iloc[0][:10]}\")\n",
    "\n",
    "# Create TaggedDocuments for Doc2Vec\n",
    "tagged_documents = [\n",
    "    TaggedDocument(words=tokens, tags=[str(idx)]) \n",
    "    for idx, tokens in enumerate(df['tokens'])\n",
    "]\n",
    "\n",
    "print(f\"\\nTotal documents for training: {len(tagged_documents)}\")\n",
    "\n",
    "# Train Doc2Vec Model\n",
    "print(\"\\n--- Training Doc2Vec Model ---\")\n",
    "print(\"This may take a few minutes...\")\n",
    "\n",
    "doc2vec_model = Doc2Vec(\n",
    "    vector_size=100,        # Dimension of embeddings\n",
    "    window=5,               # Context window\n",
    "    min_count=2,            # Ignore words that appear less than 2 times\n",
    "    workers=4,              # Parallel processing\n",
    "    epochs=40,              # Training iterations\n",
    "    dm=0,                   # PV-DBOW (faster, good for classification)\n",
    "    dbow_words=1,           # Train word vectors too\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Build vocabulary\n",
    "doc2vec_model.build_vocab(tagged_documents)\n",
    "print(f\"Vocabulary size: {len(doc2vec_model.wv)}\")\n",
    "\n",
    "# Train the model\n",
    "doc2vec_model.train(\n",
    "    tagged_documents,\n",
    "    total_examples=doc2vec_model.corpus_count,\n",
    "    epochs=doc2vec_model.epochs\n",
    ")\n",
    "\n",
    "print(\"Doc2Vec training complete!\")\n",
    "\n",
    "# Save the model\n",
    "doc2vec_model.save('doc2vec_job_model.model')\n",
    "print(\"Doc2Vec model saved as 'doc2vec_job_model.model'\")\n",
    "\n",
    "# Pre-compute job vectors\n",
    "print(\"\\n--- Pre-computing Job Vectors ---\")\n",
    "\n",
    "job_vectors = np.array([doc2vec_model.dv[str(i)] for i in range(len(df))])\n",
    "print(f\"Job vectors shape: {job_vectors.shape}\")\n",
    "\n",
    "# Save job vectors and metadata\n",
    "job_metadata = df[['ID_num', 'job_title', 'Short_description', 'Skills_required', \n",
    "                    'Industry', 'Pay_grade']].copy()\n",
    "job_metadata['vector_id'] = range(len(df))\n",
    "\n",
    "joblib.dump(job_vectors, 'job_vectors.pkl')\n",
    "joblib.dump(job_metadata, 'job_metadata.pkl')\n",
    "\n",
    "print(\"Job vectors saved as 'job_vectors.pkl'\")\n",
    "print(\"Job metadata saved as 'job_metadata.pkl'\")\n",
    "\n",
    "# Save job database as JSON for easy access\n",
    "job_database = df.to_dict('records')\n",
    "with open('job_database.json', 'w') as f:\n",
    "    json.dump(job_database, f, indent=4)\n",
    "\n",
    "print(\"Job database saved as 'job_database.json'\")\n",
    "\n",
    "# Test the model\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TESTING DOC2VEC JOB MATCHER\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def match_jobs(resume_text, top_k=10):\n",
    "    \"\"\"\n",
    "    Match jobs based on resume text\n",
    "    \n",
    "    Parameters:\n",
    "    resume_text: string containing resume content\n",
    "    top_k: number of top matching jobs to return\n",
    "    \n",
    "    Returns:\n",
    "    list of matching jobs with similarity scores\n",
    "    \"\"\"\n",
    "    # Load model and data\n",
    "    model = Doc2Vec.load('doc2vec_job_model.model')\n",
    "    vectors = joblib.load('job_vectors.pkl')\n",
    "    metadata = joblib.load('job_metadata.pkl')\n",
    "    \n",
    "    # Tokenize resume\n",
    "    resume_tokens = tokenize_document(resume_text)\n",
    "    \n",
    "    # Infer vector for resume\n",
    "    resume_vector = model.infer_vector(resume_tokens, epochs=20)\n",
    "    resume_vector = resume_vector.reshape(1, -1)\n",
    "    \n",
    "    # Calculate cosine similarity with all jobs\n",
    "    similarities = cosine_similarity(resume_vector, vectors)[0]\n",
    "    \n",
    "    # Get top K jobs\n",
    "    top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "    \n",
    "    # Prepare results\n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        job = metadata.iloc[idx]\n",
    "        results.append({\n",
    "            'job_id': int(job['ID_num']),\n",
    "            'job_title': job['job_title'],\n",
    "            'description': job['Short_description'],\n",
    "            'skills_required': job['Skills_required'],\n",
    "            'industry': job['Industry'],\n",
    "            'pay_grade': job['Pay_grade'],\n",
    "            'match_score': round(float(similarities[idx]) * 100, 2)\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test with sample resume\n",
    "test_resume = \"\"\"\n",
    "Experienced Software Engineer with 5 years in web development.\n",
    "Proficient in Python, JavaScript, and React. Strong problem-solving skills\n",
    "and experience with cloud computing platforms. Built scalable applications\n",
    "serving thousands of users. Team collaboration and critical thinking.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nTest Resume:\\n{test_resume}\\n\")\n",
    "\n",
    "matched_jobs = match_jobs(test_resume, top_k=10)\n",
    "\n",
    "print(\"--- Top 10 Matching Jobs ---\")\n",
    "for i, job in enumerate(matched_jobs, 1):\n",
    "    print(f\"\\n{i}. {job['job_title']} (Match: {job['match_score']}%)\")\n",
    "    print(f\"   Industry: {job['industry']}\")\n",
    "    print(f\"   Pay Grade: {job['pay_grade']}\")\n",
    "    print(f\"   Skills: {job['skills_required'][:80]}...\")\n",
    "    print(f\"   Description: {job['description'][:100]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DOC2VEC JOB MATCHER - COMPLETE ‚úì\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nSaved artifacts:\")\n",
    "print(\"  - doc2vec_job_model.model\")\n",
    "print(\"  - job_vectors.pkl\")\n",
    "print(\"  - job_metadata.pkl\")\n",
    "print(\"  - job_database.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PathFinder AI - Gemini Skill Extractor\n",
    "Replaces mock skill extraction with real Gemini API\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "class GeminiSkillExtractor:\n",
    "    \"\"\"Extract skills from resume using Gemini API\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key=\"AIzaSyALQl3IlQPXT_dD8k5kvBA9j3aXenmfDAg\"):\n",
    "        \"\"\"\n",
    "        Initialize Gemini client\n",
    "        \n",
    "        Parameters:\n",
    "        api_key: Gemini API key (if None, reads from environment)\n",
    "        \"\"\"\n",
    "        if api_key:\n",
    "            self.api_key = \"AIzaSyALQl3IlQPXT_dD8k5kvBA9j3aXenmfDAg\"\n",
    "        else:\n",
    "            self.api_key = os.environ.get(\"GEMINI_API_KEY\")\n",
    "        \n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"GEMINI_API_KEY not found. Set it as environment variable or pass to constructor.\")\n",
    "        \n",
    "        self.client = genai.Client(api_key=self.api_key)\n",
    "        self.model = \"gemini-flash-latest\"\n",
    "    \n",
    "    def extract_skills(self, resume_text):\n",
    "        \"\"\"\n",
    "        Extract technical and soft skills from resume\n",
    "        \n",
    "        Parameters:\n",
    "        resume_text: string containing resume content\n",
    "        \n",
    "        Returns:\n",
    "        dict with 'technical_skills' and 'soft_skills' lists\n",
    "        \"\"\"\n",
    "        \n",
    "        prompt = f\"\"\"You are a skill extraction expert for career recommendation systems.\n",
    "\n",
    "Extract ALL skills from this resume and categorize them.\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Extract ONLY actual skills (not job titles, companies, or responsibilities)\n",
    "2. Return as valid JSON with two arrays: \"technical_skills\" and \"soft_skills\"\n",
    "3. Use standard skill names (e.g., \"Machine Learning\" not \"ML\")\n",
    "4. Include programming languages, frameworks, tools, methodologies\n",
    "5. Soft skills: leadership, communication, problem-solving, etc.\n",
    "\n",
    "RESUME:\n",
    "{resume_text}\n",
    "\n",
    "OUTPUT FORMAT (JSON only, no markdown):\n",
    "{{\n",
    "  \"technical_skills\": [\"Python\", \"Machine Learning\", \"Docker\", ...],\n",
    "  \"soft_skills\": [\"Leadership\", \"Problem Solving\", \"Team Collaboration\", ...]\n",
    "}}\n",
    "\n",
    "Return ONLY the JSON object, nothing else.\"\"\"\n",
    "\n",
    "        try:\n",
    "            contents = [\n",
    "                types.Content(\n",
    "                    role=\"user\",\n",
    "                    parts=[types.Part.from_text(text=prompt)],\n",
    "                ),\n",
    "            ]\n",
    "            \n",
    "            generate_content_config = types.GenerateContentConfig(\n",
    "                temperature=0.1,  # Low temperature for consistent extraction\n",
    "                top_p=0.95,\n",
    "                top_k=40,\n",
    "                max_output_tokens=2048,\n",
    "            )\n",
    "            \n",
    "            response = self.client.models.generate_content(\n",
    "                model=self.model,\n",
    "                contents=contents,\n",
    "                config=generate_content_config,\n",
    "            )\n",
    "            \n",
    "            # Extract text from response\n",
    "            response_text = response.text.strip()\n",
    "            \n",
    "            # Remove markdown code blocks if present\n",
    "            if response_text.startswith(\"```json\"):\n",
    "                response_text = response_text.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "            elif response_text.startswith(\"```\"):\n",
    "                response_text = response_text.replace(\"```\", \"\").strip()\n",
    "            \n",
    "            # Parse JSON\n",
    "            skills_data = json.loads(response_text)\n",
    "            \n",
    "            return skills_data\n",
    "        \n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error parsing Gemini response: {e}\")\n",
    "            print(f\"Raw response: {response_text}\")\n",
    "            return {\"technical_skills\": [], \"soft_skills\": []}\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error calling Gemini API: {e}\")\n",
    "            return {\"technical_skills\": [], \"soft_skills\": []}\n",
    "    \n",
    "    def extract_skills_simple(self, resume_text):\n",
    "        \"\"\"\n",
    "        Extract all skills as a single list (for backward compatibility)\n",
    "        \n",
    "        Parameters:\n",
    "        resume_text: string containing resume content\n",
    "        \n",
    "        Returns:\n",
    "        list of all skills (technical + soft combined)\n",
    "        \"\"\"\n",
    "        skills_data = self.extract_skills(resume_text)\n",
    "        all_skills = skills_data.get('technical_skills', []) + skills_data.get('soft_skills', [])\n",
    "        return all_skills\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TESTING\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"GEMINI SKILL EXTRACTOR - TEST\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Test Resume\n",
    "    test_resume = \"\"\"\n",
    "    John Doe\n",
    "    Software Engineer\n",
    "    john.doe@email.com | LinkedIn: linkedin.com/in/johndoe\n",
    "    \n",
    "    PROFESSIONAL SUMMARY\n",
    "    Experienced Software Engineer with 5 years in full-stack development.\n",
    "    Strong problem-solving abilities and team leadership experience.\n",
    "    \n",
    "    TECHNICAL SKILLS\n",
    "    - Languages: Python, JavaScript, Java, TypeScript, SQL\n",
    "    - Frontend: React, Vue.js, HTML5, CSS3, Tailwind CSS\n",
    "    - Backend: Node.js, FastAPI, Django, Express.js\n",
    "    - Databases: PostgreSQL, MongoDB, Redis, MySQL\n",
    "    - DevOps: Docker, Kubernetes, AWS, Azure, CI/CD, Jenkins\n",
    "    - Tools: Git, GitHub Actions, Terraform, Nginx\n",
    "    - Methodologies: Agile, Scrum, Test-Driven Development\n",
    "    \n",
    "    EXPERIENCE\n",
    "    Senior Software Engineer | TechCorp Inc. | 2020 - Present\n",
    "    - Led team of 5 developers in building microservices architecture\n",
    "    - Implemented CI/CD pipelines reducing deployment time by 40%\n",
    "    - Mentored junior developers and conducted code reviews\n",
    "    \n",
    "    Software Engineer | StartupX | 2018 - 2020\n",
    "    - Built RESTful APIs serving 100k+ daily users\n",
    "    - Optimized database queries improving performance by 60%\n",
    "    - Collaborated with cross-functional teams\n",
    "    \n",
    "    EDUCATION\n",
    "    B.S. Computer Science | University of Technology | 2018\n",
    "    \n",
    "    CERTIFICATIONS\n",
    "    - AWS Certified Solutions Architect\n",
    "    - Docker Certified Associate\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Initialize extractor\n",
    "        extractor = GeminiSkillExtractor()\n",
    "        \n",
    "        print(\"\\nExtracting skills from resume...\\n\")\n",
    "        \n",
    "        # Extract skills (categorized)\n",
    "        skills = extractor.extract_skills(test_resume)\n",
    "        \n",
    "        print(\"\\n--- EXTRACTED SKILLS ---\\n\")\n",
    "        \n",
    "        print(\"üìã TECHNICAL SKILLS:\")\n",
    "        for skill in skills['technical_skills']:\n",
    "            print(f\"  ‚Ä¢ {skill}\")\n",
    "        \n",
    "        print(f\"\\nTotal Technical Skills: {len(skills['technical_skills'])}\")\n",
    "        \n",
    "        print(\"\\nüí™ SOFT SKILLS:\")\n",
    "        for skill in skills['soft_skills']:\n",
    "            print(f\"  ‚Ä¢ {skill}\")\n",
    "        \n",
    "        print(f\"\\nTotal Soft Skills: {len(skills['soft_skills'])}\")\n",
    "        \n",
    "        # Test simple extraction (combined list)\n",
    "        print(\"\\n\\n--- SIMPLE EXTRACTION (Combined) ---\\n\")\n",
    "        all_skills = extractor.extract_skills_simple(test_resume)\n",
    "        print(f\"All Skills ({len(all_skills)} total):\")\n",
    "        for skill in all_skills[:15]:  # Show first 15\n",
    "            print(f\"  ‚Ä¢ {skill}\")\n",
    "        \n",
    "        if len(all_skills) > 15:\n",
    "            print(f\"  ... and {len(all_skills) - 15} more\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"GEMINI SKILL EXTRACTOR - TEST COMPLETE ‚úì\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "    except ValueError as e:\n",
    "        print(f\"\\n‚ùå ERROR: {e}\")\n",
    "        print(\"\\nTo fix this:\")\n",
    "        print(\"1. Get Gemini API key from: https://aistudio.google.com/apikey\")\n",
    "        print(\"2. Set environment variable: export GEMINI_API_KEY='your-key-here'\")\n",
    "        print(\"3. Or pass API key to constructor: GeminiSkillExtractor(api_key='your-key')\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Unexpected error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PathFinder AI - Phase 2: Updated Combined System with Gemini\n",
    "Now uses real Gemini API for skill extraction\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import json\n",
    "import os\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# from gemini_skill_extractor import GeminiSkillExtractor\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PHASE 2: COMBINED SYSTEM WITH GEMINI API\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# COMPONENT 1: GEMINI SKILL EXTRACTION\n",
    "# ============================================================================\n",
    "\n",
    "def extract_skills_with_gemini(resume_text, api_key=\"AIzaSyALQl3IlQPXT_dD8k5kvBA9j3aXenmfDAg\"):\n",
    "    \"\"\"\n",
    "    Extract skills using Gemini API\n",
    "    \n",
    "    Parameters:\n",
    "    resume_text: string containing resume\n",
    "    api_key: optional Gemini API key\n",
    "    \n",
    "    Returns:\n",
    "    dict with technical_skills and soft_skills\n",
    "    \"\"\"\n",
    "    try:\n",
    "        extractor = GeminiSkillExtractor(api_key=\"AIzaSyALQl3IlQPXT_dD8k5kvBA9j3aXenmfDAg\")\n",
    "        skills = extractor.extract_skills(resume_text)\n",
    "        return skills\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Gemini API error: {e}\")\n",
    "        print(\"Falling back to mock extraction...\")\n",
    "        # Fallback to mock\n",
    "        return extract_skills_mock(resume_text)\n",
    "\n",
    "def extract_skills_mock(resume_text):\n",
    "    \"\"\"Fallback mock skill extraction\"\"\"\n",
    "    skills_keywords = [\n",
    "        'Python', 'Java', 'JavaScript', 'React', 'Node.js', 'SQL', 'MongoDB',\n",
    "        'Machine Learning', 'Data Analysis', 'Deep Learning', 'TensorFlow',\n",
    "        'PyTorch', 'Docker', 'Kubernetes', 'AWS', 'Azure', 'Cloud Computing',\n",
    "        'System Design', 'Problem Solving', 'Critical Thinking', 'FastAPI',\n",
    "        'Flask', 'Django', 'PostgreSQL', 'Redis', 'Microservices', 'REST API',\n",
    "        'GraphQL', 'HTML', 'CSS', 'TypeScript', 'Git', 'CI/CD', 'Agile',\n",
    "        'Team Collaboration', 'Communication', 'Leadership', 'Data Science',\n",
    "        'Statistics', 'Research', 'Analytical Thinking', 'Pattern Recognition'\n",
    "    ]\n",
    "    \n",
    "    extracted = []\n",
    "    resume_lower = resume_text.lower()\n",
    "    \n",
    "    for skill in skills_keywords:\n",
    "        if skill.lower() in resume_lower:\n",
    "            extracted.append(skill)\n",
    "    \n",
    "    return {\n",
    "        'technical_skills': extracted,\n",
    "        'soft_skills': []\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# COMPONENT 2: KNN CAREER RECOMMENDATION\n",
    "# ============================================================================\n",
    "\n",
    "def recommend_careers(user_skills, top_k=5):\n",
    "    \"\"\"Recommend careers using KNN\"\"\"\n",
    "    try:\n",
    "        knn = joblib.load('/kaggle/input/ai-career-recommendation-system/knn_career_model.pkl')\n",
    "        mlb = joblib.load('/kaggle/input/ai-career-recommendation-system/skills_mlb.pkl')\n",
    "        career_ref = joblib.load('/kaggle/input/ai-career-recommendation-system/career_reference.pkl')\n",
    "        \n",
    "        user_skills_encoded = mlb.transform([user_skills])\n",
    "        distances, indices = knn.kneighbors(\n",
    "            user_skills_encoded, \n",
    "            n_neighbors=min(top_k, len(career_ref))\n",
    "        )\n",
    "        \n",
    "        recommendations = []\n",
    "        for dist, idx in zip(distances[0], indices[0]):\n",
    "            similarity = 1 - dist\n",
    "            career = career_ref.iloc[idx]['Career']\n",
    "            career_skills = career_ref.iloc[idx]['Skills']\n",
    "            \n",
    "            matching_skills = set(user_skills) & set(career_skills)\n",
    "            missing_skills = set(career_skills) - set(user_skills)\n",
    "            \n",
    "            recommendations.append({\n",
    "                'career': career,\n",
    "                'similarity_score': round(similarity * 100, 2),\n",
    "                'matching_skills': list(matching_skills),\n",
    "                'missing_skills': list(missing_skills)[:5],\n",
    "                'total_required_skills': len(career_skills)\n",
    "            })\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in career recommendation: {e}\")\n",
    "        return []\n",
    "\n",
    "# ============================================================================\n",
    "# COMPONENT 3: DOC2VEC JOB MATCHING\n",
    "# ============================================================================\n",
    "\n",
    "def tokenize_document(doc):\n",
    "    \"\"\"Tokenize document\"\"\"\n",
    "    return simple_preprocess(doc, deacc=True, min_len=2, max_len=15)\n",
    "\n",
    "def match_jobs(resume_text, top_k=10, filters=None):\n",
    "    \"\"\"Match jobs using Doc2Vec\"\"\"\n",
    "    try:\n",
    "        model = Doc2Vec.load('doc2vec_job_model.model')\n",
    "        vectors = joblib.load('job_vectors.pkl')\n",
    "        metadata = joblib.load('job_metadata.pkl')\n",
    "        \n",
    "        if filters:\n",
    "            mask = pd.Series([True] * len(metadata))\n",
    "            \n",
    "            if 'industry' in filters and filters['industry']:\n",
    "                mask &= metadata['Industry'] == filters['industry']\n",
    "            \n",
    "            if 'pay_grade' in filters and filters['pay_grade']:\n",
    "                mask &= metadata['Pay_grade'] == filters['pay_grade']\n",
    "            \n",
    "            filtered_metadata = metadata[mask].reset_index(drop=True)\n",
    "            filtered_vectors = vectors[mask.values]\n",
    "        else:\n",
    "            filtered_metadata = metadata\n",
    "            filtered_vectors = vectors\n",
    "        \n",
    "        if len(filtered_metadata) == 0:\n",
    "            return []\n",
    "        \n",
    "        resume_tokens = tokenize_document(resume_text)\n",
    "        resume_vector = model.infer_vector(resume_tokens, epochs=20)\n",
    "        resume_vector = resume_vector.reshape(1, -1)\n",
    "        \n",
    "        similarities = cosine_similarity(resume_vector, filtered_vectors)[0]\n",
    "        \n",
    "        top_k = min(top_k, len(similarities))\n",
    "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "        \n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            job = filtered_metadata.iloc[idx]\n",
    "            results.append({\n",
    "                'job_id': int(job['ID_num']),\n",
    "                'job_title': job['job_title'],\n",
    "                'description': job['Short_description'],\n",
    "                'skills_required': job['Skills_required'],\n",
    "                'industry': job['Industry'],\n",
    "                'pay_grade': job['Pay_grade'],\n",
    "                'match_score': round(float(similarities[idx]) * 100, 2)\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in job matching: {e}\")\n",
    "        return []\n",
    "\n",
    "# ============================================================================\n",
    "# COMPONENT 4: COMBINED PIPELINE WITH GEMINI\n",
    "# ============================================================================\n",
    "\n",
    "def complete_recommendation_pipeline(resume_text, top_careers=5, top_jobs=10, \n",
    "                                     job_filters=None, use_gemini=True, api_key=None):\n",
    "    \"\"\"\n",
    "    Complete pipeline with Gemini skill extraction\n",
    "    \n",
    "    Parameters:\n",
    "    resume_text: resume content\n",
    "    top_careers: number of career recommendations\n",
    "    top_jobs: number of job matches\n",
    "    job_filters: optional filters (industry, pay_grade)\n",
    "    use_gemini: if True, use Gemini API; if False, use mock\n",
    "    api_key: optional Gemini API key\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"RUNNING COMPLETE PIPELINE WITH GEMINI\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Step 1: Extract skills\n",
    "    print(\"\\n[Step 1] Extracting skills from resume...\")\n",
    "    \n",
    "    if use_gemini:\n",
    "        print(\"Using Gemini API...\")\n",
    "        skills_data = extract_skills_with_gemini(resume_text, api_key=api_key)\n",
    "    else:\n",
    "        print(\"Using mock extraction...\")\n",
    "        skills_data = extract_skills_mock(resume_text)\n",
    "    \n",
    "    technical_skills = skills_data.get('technical_skills', [])\n",
    "    soft_skills = skills_data.get('soft_skills', [])\n",
    "    all_skills = technical_skills + soft_skills\n",
    "    \n",
    "    print(f\"Extracted {len(technical_skills)} technical skills, {len(soft_skills)} soft skills\")\n",
    "    \n",
    "    # Step 2: Career recommendations\n",
    "    print(\"\\n[Step 2] Finding matching careers...\")\n",
    "    career_recommendations = recommend_careers(all_skills, top_k=top_careers)\n",
    "    \n",
    "    # Step 3: Job matching\n",
    "    print(\"\\n[Step 3] Matching jobs to resume...\")\n",
    "    job_matches = match_jobs(resume_text, top_k=top_jobs, filters=job_filters)\n",
    "    \n",
    "    results = {\n",
    "        'technical_skills': technical_skills,\n",
    "        'soft_skills': soft_skills,\n",
    "        'all_skills': all_skills,\n",
    "        'recommended_careers': career_recommendations,\n",
    "        'matched_jobs': job_matches\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def display_results(results):\n",
    "    \"\"\"Display results in formatted output\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"RECOMMENDATION RESULTS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Technical Skills\n",
    "    print(\"\\nüíª TECHNICAL SKILLS:\")\n",
    "    for skill in results['technical_skills'][:15]:  # Show first 15\n",
    "        print(f\"  ‚Ä¢ {skill}\")\n",
    "    if len(results['technical_skills']) > 15:\n",
    "        print(f\"  ... and {len(results['technical_skills']) - 15} more\")\n",
    "    \n",
    "    # Soft Skills\n",
    "    print(\"\\nüí™ SOFT SKILLS:\")\n",
    "    for skill in results['soft_skills'][:10]:  # Show first 10\n",
    "        print(f\"  ‚Ä¢ {skill}\")\n",
    "    if len(results['soft_skills']) > 10:\n",
    "        print(f\"  ... and {len(results['soft_skills']) - 10} more\")\n",
    "    \n",
    "    # Career Recommendations\n",
    "    print(\"\\nüéØ RECOMMENDED CAREERS:\")\n",
    "    for i, career in enumerate(results['recommended_careers'], 1):\n",
    "        print(f\"\\n{i}. {career['career']} ({career['similarity_score']}% match)\")\n",
    "        if career['matching_skills']:\n",
    "            print(f\"   Matching Skills: {', '.join(career['matching_skills'][:5])}\")\n",
    "        if career['missing_skills']:\n",
    "            print(f\"   Skills to Learn: {', '.join(career['missing_skills'][:3])}\")\n",
    "    \n",
    "    # Job Matches\n",
    "    print(\"\\nüíº MATCHING JOBS:\")\n",
    "    for i, job in enumerate(results['matched_jobs'], 1):\n",
    "        print(f\"\\n{i}. {job['job_title']} ({job['match_score']}% match)\")\n",
    "        print(f\"   Industry: {job['industry']} | Pay: {job['pay_grade']}\")\n",
    "        print(f\"   Skills: {job['skills_required'][:70]}...\")\n",
    "\n",
    "# ============================================================================\n",
    "# TESTING\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Test Resume\n",
    "    test_resume = \"\"\"\n",
    "    Sarah Chen\n",
    "    Full-Stack Developer\n",
    "    sarah.chen@email.com | Portfolio: sarahchen.dev\n",
    "    \n",
    "    SUMMARY\n",
    "    Full-stack developer with 4 years experience building scalable web applications.\n",
    "    Passionate about clean code and user experience. Strong problem-solving skills.\n",
    "    \n",
    "    TECHNICAL SKILLS\n",
    "    Frontend: React, Next.js, TypeScript, HTML5, CSS3, Tailwind CSS\n",
    "    Backend: Node.js, Python, FastAPI, Express.js, Django\n",
    "    Databases: PostgreSQL, MongoDB, Redis\n",
    "    Cloud & DevOps: AWS, Docker, Kubernetes, CI/CD, GitHub Actions\n",
    "    Tools: Git, Jest, Cypress, Webpack\n",
    "    \n",
    "    SOFT SKILLS\n",
    "    Team collaboration, leadership, communication, critical thinking, time management\n",
    "    \n",
    "    EXPERIENCE\n",
    "    Senior Developer | WebCorp | 2021 - Present\n",
    "    - Led development of e-commerce platform serving 50k users\n",
    "    - Implemented microservices architecture with Docker\n",
    "    - Mentored 3 junior developers\n",
    "    - Improved page load time by 60%\n",
    "    \n",
    "    Developer | StartupHub | 2019 - 2021\n",
    "    - Built RESTful APIs with Node.js and PostgreSQL\n",
    "    - Developed responsive React applications\n",
    "    - Collaborated with cross-functional teams\n",
    "    \n",
    "    EDUCATION\n",
    "    B.S. Computer Science | Tech University | 2019\n",
    "    \n",
    "    CERTIFICATIONS\n",
    "    - AWS Certified Developer\n",
    "    - React Advanced Certification\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TEST: FULL-STACK DEVELOPER PROFILE\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Check if Gemini API key is available\n",
    "    gemini_key = \"AIzaSyALQl3IlQPXT_dD8k5kvBA9j3aXenmfDAg\"\n",
    "    use_gemini = bool(gemini_key)\n",
    "    \n",
    "    if use_gemini:\n",
    "        print(\"\\n‚úì Gemini API key found - Using real Gemini extraction\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è Gemini API key not found - Using mock extraction\")\n",
    "        print(\"Set GEMINI_API_KEY environment variable to use real extraction\")\n",
    "    \n",
    "    # Run pipeline\n",
    "    results = complete_recommendation_pipeline(\n",
    "        resume_text=test_resume,\n",
    "        top_careers=5,\n",
    "        top_jobs=10,\n",
    "        use_gemini=use_gemini\n",
    "    )\n",
    "    \n",
    "    display_results(results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PHASE 2 WITH GEMINI - COMPLETE ‚úì\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if use_gemini:\n",
    "        print(\"\\n‚úì Gemini API skill extraction: WORKING\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è Gemini API: NOT CONFIGURED (using mock)\")\n",
    "        print(\"To enable: export GEMINI_API_KEY='your-key-here'\")\n",
    "    \n",
    "    print(\"‚úì KNN Career Recommendation: WORKING\")\n",
    "    print(\"‚úì Doc2Vec Job Matching: WORKING\")\n",
    "    print(\"‚úì Combined Pipeline: WORKING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PathFinder AI - Phase 3: Gemini Roadmap Generator\n",
    "Generates 3-4 personalized career roadmap variants\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "class GeminiRoadmapGenerator:\n",
    "    \"\"\"Generate career roadmaps using Gemini API\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key=\"AIzaSyALQl3IlQPXT_dD8k5kvBA9j3aXenmfDAg\"):\n",
    "        \"\"\"Initialize Gemini client\"\"\"\n",
    "        if api_key:\n",
    "            self.api_key = \"AIzaSyALQl3IlQPXT_dD8k5kvBA9j3aXenmfDAg\"\n",
    "        else:\n",
    "            self.api_key = os.environ.get(\"GEMINI_API_KEY\")\n",
    "        \n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"GEMINI_API_KEY not found\")\n",
    "        \n",
    "        self.client = genai.Client(api_key=self.api_key)\n",
    "        self.model = \"gemini-flash-latest\"\n",
    "    \n",
    "    def generate_roadmaps(self, user_profile):\n",
    "        \"\"\"\n",
    "        Generate 3-4 roadmap variants for user\n",
    "        \n",
    "        Parameters:\n",
    "        user_profile: dict containing:\n",
    "            - target_career: str (e.g., \"Data Scientist\")\n",
    "            - current_skills: list (e.g., [\"Python\", \"SQL\"])\n",
    "            - missing_skills: list (e.g., [\"Machine Learning\", \"Statistics\"])\n",
    "            - experience_level: str (e.g., \"beginner\", \"intermediate\", \"advanced\")\n",
    "            - time_commitment: str (e.g., \"full-time\", \"part-time\", \"weekends\")\n",
    "        \n",
    "        Returns:\n",
    "        list of 4 roadmap variants, each with different learning paths\n",
    "        \"\"\"\n",
    "        \n",
    "        target_career = user_profile.get('target_career', 'Software Engineer')\n",
    "        current_skills = user_profile.get('current_skills', [])\n",
    "        missing_skills = user_profile.get('missing_skills', [])\n",
    "        experience_level = user_profile.get('experience_level', 'intermediate')\n",
    "        time_commitment = user_profile.get('time_commitment', 'part-time')\n",
    "        \n",
    "        prompt = f\"\"\"You are a career roadmap expert. Generate 4 DIFFERENT learning roadmaps for someone to become a {target_career}.\n",
    "\n",
    "USER PROFILE:\n",
    "- Target Career: {target_career}\n",
    "- Current Skills: {', '.join(current_skills) if current_skills else 'None'}\n",
    "- Skills to Learn: {', '.join(missing_skills) if missing_skills else 'Various'}\n",
    "- Experience Level: {experience_level}\n",
    "- Time Commitment: {time_commitment}\n",
    "\n",
    "REQUIREMENTS:\n",
    "1. Create 4 DISTINCT roadmap variants with different approaches:\n",
    "   - Variant 1: Fast-track intensive path (aggressive timeline)\n",
    "   - Variant 2: Balanced structured path (moderate timeline)\n",
    "   - Variant 3: Self-paced flexible path (relaxed timeline)\n",
    "   - Variant 4: Project-based practical path (learning by doing)\n",
    "\n",
    "2. Each roadmap must have:\n",
    "   - roadmap_name: descriptive name\n",
    "   - description: 1-2 sentence overview\n",
    "   - duration_months: estimated time to complete\n",
    "   - difficulty: \"beginner\", \"intermediate\", or \"advanced\"\n",
    "   - steps: array of 5-8 learning steps\n",
    "\n",
    "3. Each step must have:\n",
    "   - step_number: 1, 2, 3...\n",
    "   - title: clear step name\n",
    "   - description: what to learn/do\n",
    "   - duration_weeks: time for this step\n",
    "   - resources: 2-3 specific learning resources (courses, books, projects)\n",
    "   - skills_gained: list of skills learned in this step\n",
    "\n",
    "OUTPUT FORMAT (JSON only):\n",
    "{{\n",
    "  \"roadmaps\": [\n",
    "    {{\n",
    "      \"roadmap_id\": 1,\n",
    "      \"roadmap_name\": \"Fast-Track Intensive Path\",\n",
    "      \"description\": \"Aggressive 6-month bootcamp-style learning\",\n",
    "      \"duration_months\": 6,\n",
    "      \"difficulty\": \"intermediate\",\n",
    "      \"steps\": [\n",
    "        {{\n",
    "          \"step_number\": 1,\n",
    "          \"title\": \"Master Python Fundamentals\",\n",
    "          \"description\": \"Learn Python syntax, data structures, OOP\",\n",
    "          \"duration_weeks\": 3,\n",
    "          \"resources\": [\"Python Crash Course (book)\", \"Codecademy Python\", \"100 Days of Code\"],\n",
    "          \"skills_gained\": [\"Python\", \"Data Structures\", \"Algorithms\"]\n",
    "        }},\n",
    "        ...\n",
    "      ]\n",
    "    }},\n",
    "    ... 3 more roadmaps\n",
    "  ]\n",
    "}}\n",
    "\n",
    "Return ONLY valid JSON, no markdown or extra text.\"\"\"\n",
    "\n",
    "        try:\n",
    "            contents = [\n",
    "                types.Content(\n",
    "                    role=\"user\",\n",
    "                    parts=[types.Part.from_text(text=prompt)],\n",
    "                ),\n",
    "            ]\n",
    "            \n",
    "            generate_content_config = types.GenerateContentConfig(\n",
    "                temperature=0.7,  # Higher for creative variety\n",
    "                top_p=0.95,\n",
    "                top_k=40,\n",
    "                max_output_tokens=4096,\n",
    "            )\n",
    "            \n",
    "            response = self.client.models.generate_content(\n",
    "                model=self.model,\n",
    "                contents=contents,\n",
    "                config=generate_content_config,\n",
    "            )\n",
    "            \n",
    "            response_text = response.text.strip()\n",
    "            \n",
    "            # Clean markdown\n",
    "            if response_text.startswith(\"```json\"):\n",
    "                response_text = response_text.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "            elif response_text.startswith(\"```\"):\n",
    "                response_text = response_text.replace(\"```\", \"\").strip()\n",
    "            \n",
    "            # Parse JSON\n",
    "            roadmaps_data = json.loads(response_text)\n",
    "            \n",
    "            return roadmaps_data.get('roadmaps', [])\n",
    "        \n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error parsing Gemini response: {e}\")\n",
    "            print(f\"Raw response: {response_text[:500]}...\")\n",
    "            return []\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error calling Gemini API: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def analyze_skill_gap(self, user_skills, required_skills):\n",
    "        \"\"\"\n",
    "        Analyze skill gap between current and required skills\n",
    "        \n",
    "        Parameters:\n",
    "        user_skills: list of current skills\n",
    "        required_skills: list of required skills for career\n",
    "        \n",
    "        Returns:\n",
    "        dict with gap analysis\n",
    "        \"\"\"\n",
    "        \n",
    "        prompt = f\"\"\"Analyze the skill gap for career transition.\n",
    "\n",
    "CURRENT SKILLS:\n",
    "{', '.join(user_skills) if user_skills else 'None'}\n",
    "\n",
    "REQUIRED SKILLS:\n",
    "{', '.join(required_skills)}\n",
    "\n",
    "TASK:\n",
    "1. Identify which current skills are transferable\n",
    "2. List skills that need to be learned\n",
    "3. Prioritize missing skills (high/medium/low priority)\n",
    "4. Estimate learning time for each missing skill\n",
    "\n",
    "OUTPUT FORMAT (JSON only):\n",
    "{{\n",
    "  \"transferable_skills\": [\"skill1\", \"skill2\"],\n",
    "  \"missing_skills\": [\n",
    "    {{\n",
    "      \"skill\": \"Machine Learning\",\n",
    "      \"priority\": \"high\",\n",
    "      \"learning_time_weeks\": 8,\n",
    "      \"difficulty\": \"intermediate\",\n",
    "      \"reason\": \"Core requirement for data science roles\"\n",
    "    }},\n",
    "    ...\n",
    "  ],\n",
    "  \"learning_path_summary\": \"Brief 2-3 sentence summary of recommended approach\"\n",
    "}}\n",
    "\n",
    "Return ONLY valid JSON.\"\"\"\n",
    "\n",
    "        try:\n",
    "            contents = [\n",
    "                types.Content(\n",
    "                    role=\"user\",\n",
    "                    parts=[types.Part.from_text(text=prompt)],\n",
    "                ),\n",
    "            ]\n",
    "            \n",
    "            generate_content_config = types.GenerateContentConfig(\n",
    "                temperature=0.3,\n",
    "                max_output_tokens=2048,\n",
    "            )\n",
    "            \n",
    "            response = self.client.models.generate_content(\n",
    "                model=self.model,\n",
    "                contents=contents,\n",
    "                config=generate_content_config,\n",
    "            )\n",
    "            \n",
    "            response_text = response.text.strip()\n",
    "            \n",
    "            if response_text.startswith(\"```json\"):\n",
    "                response_text = response_text.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "            elif response_text.startswith(\"```\"):\n",
    "                response_text = response_text.replace(\"```\", \"\").strip()\n",
    "            \n",
    "            gap_analysis = json.loads(response_text)\n",
    "            \n",
    "            return gap_analysis\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error in skill gap analysis: {e}\")\n",
    "            return {\n",
    "                \"transferable_skills\": [],\n",
    "                \"missing_skills\": [],\n",
    "                \"learning_path_summary\": \"Unable to analyze skill gap\"\n",
    "            }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TESTING\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"PHASE 3: GEMINI ROADMAP GENERATOR - TEST\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    try:\n",
    "        generator = GeminiRoadmapGenerator()\n",
    "        \n",
    "        # Test Case 1: Data Scientist Career\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"TEST 1: DATA SCIENTIST ROADMAP\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        user_profile_1 = {\n",
    "            'target_career': 'Data Scientist',\n",
    "            'current_skills': ['Python', 'SQL', 'Excel'],\n",
    "            'missing_skills': ['Machine Learning', 'Statistics', 'Deep Learning', 'TensorFlow'],\n",
    "            'experience_level': 'beginner',\n",
    "            'time_commitment': 'part-time'\n",
    "        }\n",
    "        \n",
    "        print(\"\\nGenerating roadmaps...\")\n",
    "        roadmaps_1 = generator.generate_roadmaps(user_profile_1)\n",
    "        \n",
    "        print(f\"\\n‚úì Generated {len(roadmaps_1)} roadmap variants\\n\")\n",
    "        \n",
    "        for roadmap in roadmaps_1:\n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(f\"ROADMAP {roadmap['roadmap_id']}: {roadmap['roadmap_name']}\")\n",
    "            print(f\"{'='*70}\")\n",
    "            print(f\"Description: {roadmap['description']}\")\n",
    "            print(f\"Duration: {roadmap['duration_months']} months\")\n",
    "            print(f\"Difficulty: {roadmap['difficulty']}\")\n",
    "            print(f\"\\nSteps ({len(roadmap['steps'])} total):\")\n",
    "            \n",
    "            for step in roadmap['steps'][:3]:  # Show first 3 steps\n",
    "                print(f\"\\n  Step {step['step_number']}: {step['title']}\")\n",
    "                print(f\"  Duration: {step['duration_weeks']} weeks\")\n",
    "                print(f\"  Skills: {', '.join(step['skills_gained'])}\")\n",
    "                print(f\"  Resources: {', '.join(step['resources'][:2])}\")\n",
    "            \n",
    "            if len(roadmap['steps']) > 3:\n",
    "                print(f\"\\n  ... and {len(roadmap['steps']) - 3} more steps\")\n",
    "        \n",
    "        # Test Case 2: Skill Gap Analysis\n",
    "        print(\"\\n\\n\" + \"=\"*70)\n",
    "        print(\"TEST 2: SKILL GAP ANALYSIS\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        print(\"\\nAnalyzing skill gap...\")\n",
    "        gap_analysis = generator.analyze_skill_gap(\n",
    "            user_skills=['Python', 'SQL', 'Excel'],\n",
    "            required_skills=['Python', 'SQL', 'Machine Learning', 'Statistics', \n",
    "                           'Deep Learning', 'TensorFlow', 'Data Visualization']\n",
    "        )\n",
    "        \n",
    "        print(\"\\n--- SKILL GAP ANALYSIS ---\\n\")\n",
    "        \n",
    "        print(\"‚úì TRANSFERABLE SKILLS:\")\n",
    "        for skill in gap_analysis.get('transferable_skills', []):\n",
    "            print(f\"  ‚Ä¢ {skill}\")\n",
    "        \n",
    "        print(\"\\nüìö SKILLS TO LEARN:\")\n",
    "        for skill_info in gap_analysis.get('missing_skills', [])[:5]:\n",
    "            print(f\"\\n  ‚Ä¢ {skill_info['skill']}\")\n",
    "            print(f\"    Priority: {skill_info['priority']}\")\n",
    "            print(f\"    Time: {skill_info['learning_time_weeks']} weeks\")\n",
    "            print(f\"    Difficulty: {skill_info['difficulty']}\")\n",
    "        \n",
    "        print(f\"\\nüí° SUMMARY:\")\n",
    "        print(f\"  {gap_analysis.get('learning_path_summary', 'N/A')}\")\n",
    "        \n",
    "        # Save roadmaps to JSON\n",
    "        output_data = {\n",
    "            'user_profile': user_profile_1,\n",
    "            'roadmaps': roadmaps_1,\n",
    "            'skill_gap_analysis': gap_analysis\n",
    "        }\n",
    "        \n",
    "        with open('generated_roadmaps.json', 'w') as f:\n",
    "            json.dump(output_data, f, indent=2)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"GEMINI ROADMAP GENERATOR - TEST COMPLETE ‚úì\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"\\nOutputs:\")\n",
    "        print(\"  ‚úì 4 roadmap variants generated\")\n",
    "        print(\"  ‚úì Skill gap analysis complete\")\n",
    "        print(\"  ‚úì Results saved to generated_roadmaps.json\")\n",
    "        \n",
    "    except ValueError as e:\n",
    "        print(f\"\\n‚ùå ERROR: {e}\")\n",
    "        print(\"\\nTo fix: Set GEMINI_API_KEY environment variable\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Unexpected error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PathFinder AI - Phase 3: Contextual Bandits (RL Algorithm #1)\n",
    "Selects best roadmap variant from Gemini's 3-4 options\n",
    "Learns from user feedback over time\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "class ContextualBandit:\n",
    "    \"\"\"\n",
    "    Contextual Multi-Armed Bandit for roadmap selection\n",
    "    Uses epsilon-greedy strategy with UCB (Upper Confidence Bound)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_arms=4, epsilon=0.1, alpha=0.1):\n",
    "        \"\"\"\n",
    "        Initialize Contextual Bandit\n",
    "        \n",
    "        Parameters:\n",
    "        n_arms: number of roadmap variants (usually 4)\n",
    "        epsilon: exploration rate (10% random, 90% best)\n",
    "        alpha: learning rate for updates\n",
    "        \"\"\"\n",
    "        self.n_arms = n_arms\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # Track performance of each arm (roadmap variant)\n",
    "        self.q_values = np.zeros(n_arms)  # Estimated value of each arm\n",
    "        self.arm_counts = np.zeros(n_arms)  # How many times each arm selected\n",
    "        self.total_rewards = np.zeros(n_arms)  # Total reward per arm\n",
    "        \n",
    "        # Context-aware: track performance by user type\n",
    "        self.context_history = defaultdict(lambda: {\n",
    "            'q_values': np.zeros(n_arms),\n",
    "            'counts': np.zeros(n_arms),\n",
    "            'rewards': np.zeros(n_arms)\n",
    "        })\n",
    "        \n",
    "        self.total_selections = 0\n",
    "    \n",
    "    def get_context_key(self, user_context):\n",
    "        \"\"\"\n",
    "        Create context key from user profile\n",
    "        \n",
    "        Parameters:\n",
    "        user_context: dict with user info (experience_level, time_commitment, etc.)\n",
    "        \n",
    "        Returns:\n",
    "        string key for context\n",
    "        \"\"\"\n",
    "        exp_level = user_context.get('experience_level', 'intermediate')\n",
    "        time_commit = user_context.get('time_commitment', 'part-time')\n",
    "        return f\"{exp_level}_{time_commit}\"\n",
    "    \n",
    "    def select_arm(self, user_context=None):\n",
    "        \"\"\"\n",
    "        Select which roadmap variant to show\n",
    "        \n",
    "        Parameters:\n",
    "        user_context: dict with user profile info\n",
    "        \n",
    "        Returns:\n",
    "        arm_id: which roadmap to show (0-3)\n",
    "        \"\"\"\n",
    "        # Epsilon-greedy: explore vs exploit\n",
    "        if np.random.random() < self.epsilon:\n",
    "            # Exploration: random selection\n",
    "            return np.random.randint(0, self.n_arms)\n",
    "        else:\n",
    "            # Exploitation: select best arm\n",
    "            if user_context:\n",
    "                context_key = self.get_context_key(user_context)\n",
    "                context_data = self.context_history[context_key]\n",
    "                \n",
    "                # Use UCB (Upper Confidence Bound) for selection\n",
    "                if np.sum(context_data['counts']) == 0:\n",
    "                    return np.random.randint(0, self.n_arms)\n",
    "                \n",
    "                ucb_values = context_data['q_values'] + np.sqrt(\n",
    "                    2 * np.log(np.sum(context_data['counts']) + 1) / \n",
    "                    (context_data['counts'] + 1e-5)\n",
    "                )\n",
    "                return int(np.argmax(ucb_values))\n",
    "            else:\n",
    "                # No context: use global Q-values\n",
    "                return int(np.argmax(self.q_values))\n",
    "    \n",
    "    def update(self, arm_id, reward, user_context=None):\n",
    "        \"\"\"\n",
    "        Update arm values based on user feedback\n",
    "        \n",
    "        Parameters:\n",
    "        arm_id: which roadmap was shown (0-3)\n",
    "        reward: user feedback score (e.g., 1-5 stars, or 0/1 for like/dislike)\n",
    "        user_context: optional user profile\n",
    "        \"\"\"\n",
    "        # Normalize reward to 0-1 range\n",
    "        normalized_reward = reward / 5.0 if reward <= 5 else reward\n",
    "        \n",
    "        # Update global statistics\n",
    "        self.arm_counts[arm_id] += 1\n",
    "        self.total_rewards[arm_id] += normalized_reward\n",
    "        self.q_values[arm_id] = self.total_rewards[arm_id] / self.arm_counts[arm_id]\n",
    "        \n",
    "        # Update context-specific statistics\n",
    "        if user_context:\n",
    "            context_key = self.get_context_key(user_context)\n",
    "            context_data = self.context_history[context_key]\n",
    "            \n",
    "            context_data['counts'][arm_id] += 1\n",
    "            context_data['rewards'][arm_id] += normalized_reward\n",
    "            context_data['q_values'][arm_id] = (\n",
    "                context_data['rewards'][arm_id] / context_data['counts'][arm_id]\n",
    "            )\n",
    "        \n",
    "        self.total_selections += 1\n",
    "    \n",
    "    def get_statistics(self):\n",
    "        \"\"\"Get current bandit statistics\"\"\"\n",
    "        return {\n",
    "            'q_values': self.q_values.tolist(),\n",
    "            'arm_counts': self.arm_counts.tolist(),\n",
    "            'total_rewards': self.total_rewards.tolist(),\n",
    "            'total_selections': int(self.total_selections),\n",
    "            'best_arm': int(np.argmax(self.q_values)),\n",
    "            'context_stats': {\n",
    "                k: {\n",
    "                    'q_values': v['q_values'].tolist(),\n",
    "                    'counts': v['counts'].tolist()\n",
    "                }\n",
    "                for k, v in self.context_history.items()\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def save(self, filepath='contextual_bandit.pkl'):\n",
    "        \"\"\"Save bandit state\"\"\"\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'q_values': self.q_values,\n",
    "                'arm_counts': self.arm_counts,\n",
    "                'total_rewards': self.total_rewards,\n",
    "                'context_history': dict(self.context_history),\n",
    "                'total_selections': self.total_selections,\n",
    "                'n_arms': self.n_arms,\n",
    "                'epsilon': self.epsilon,\n",
    "                'alpha': self.alpha\n",
    "            }, f)\n",
    "        print(f\"Contextual Bandit saved to {filepath}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, filepath='contextual_bandit.pkl'):\n",
    "        \"\"\"Load bandit state\"\"\"\n",
    "        with open(filepath, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        \n",
    "        bandit = cls(\n",
    "            n_arms=data['n_arms'],\n",
    "            epsilon=data['epsilon'],\n",
    "            alpha=data['alpha']\n",
    "        )\n",
    "        bandit.q_values = data['q_values']\n",
    "        bandit.arm_counts = data['arm_counts']\n",
    "        bandit.total_rewards = data['total_rewards']\n",
    "        bandit.context_history = defaultdict(lambda: {\n",
    "            'q_values': np.zeros(data['n_arms']),\n",
    "            'counts': np.zeros(data['n_arms']),\n",
    "            'rewards': np.zeros(data['n_arms'])\n",
    "        }, data['context_history'])\n",
    "        bandit.total_selections = data['total_selections']\n",
    "        \n",
    "        print(f\"Contextual Bandit loaded from {filepath}\")\n",
    "        return bandit\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SYNTHETIC DATA GENERATOR (for initial training)\n",
    "# ============================================================================\n",
    "\n",
    "def generate_synthetic_feedback(n_samples=100):\n",
    "    \"\"\"\n",
    "    Generate synthetic user feedback for initial training\n",
    "    \n",
    "    Simulates user preferences:\n",
    "    - Beginners prefer slower paces (Variant 2, 3)\n",
    "    - Advanced users prefer fast-track (Variant 1)\n",
    "    - Project-based (Variant 4) universally liked\n",
    "    \"\"\"\n",
    "    synthetic_data = []\n",
    "    \n",
    "    user_types = [\n",
    "        {'experience_level': 'beginner', 'time_commitment': 'part-time'},\n",
    "        {'experience_level': 'beginner', 'time_commitment': 'full-time'},\n",
    "        {'experience_level': 'intermediate', 'time_commitment': 'part-time'},\n",
    "        {'experience_level': 'intermediate', 'time_commitment': 'full-time'},\n",
    "        {'experience_level': 'advanced', 'time_commitment': 'part-time'},\n",
    "        {'experience_level': 'advanced', 'time_commitment': 'full-time'},\n",
    "    ]\n",
    "    \n",
    "    # Simulated preferences (reward probabilities for each arm)\n",
    "    preferences = {\n",
    "        'beginner_part-time': [0.3, 0.7, 0.8, 0.6],  # Prefers Variant 2,3\n",
    "        'beginner_full-time': [0.5, 0.8, 0.6, 0.7],  # Prefers Variant 2\n",
    "        'intermediate_part-time': [0.6, 0.7, 0.7, 0.8],  # Balanced\n",
    "        'intermediate_full-time': [0.7, 0.7, 0.5, 0.8],  # Prefers 1,4\n",
    "        'advanced_part-time': [0.8, 0.5, 0.4, 0.7],  # Prefers Variant 1\n",
    "        'advanced_full-time': [0.9, 0.5, 0.3, 0.7],  # Strongly prefers 1\n",
    "    }\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        user_context = np.random.choice([ut for ut in user_types])\n",
    "        context_key = f\"{user_context['experience_level']}_{user_context['time_commitment']}\"\n",
    "        \n",
    "        # Each arm shown with equal probability initially\n",
    "        arm = np.random.randint(0, 4)\n",
    "        \n",
    "        # Generate reward based on preferences\n",
    "        reward_prob = preferences[context_key][arm]\n",
    "        reward = 5 if np.random.random() < reward_prob else np.random.randint(1, 4)\n",
    "        \n",
    "        synthetic_data.append({\n",
    "            'user_context': user_context,\n",
    "            'arm_selected': arm,\n",
    "            'reward': reward\n",
    "        })\n",
    "    \n",
    "    return synthetic_data\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TESTING\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"PHASE 3: CONTEXTUAL BANDITS - TEST\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Initialize bandit\n",
    "    print(\"\\n[Step 1] Initializing Contextual Bandit...\")\n",
    "    bandit = ContextualBandit(n_arms=4, epsilon=0.1)\n",
    "    print(\"‚úì Bandit initialized with 4 arms (roadmap variants)\")\n",
    "    \n",
    "    # Generate synthetic training data\n",
    "    print(\"\\n[Step 2] Generating synthetic user feedback...\")\n",
    "    synthetic_data = generate_synthetic_feedback(n_samples=200)\n",
    "    print(f\"‚úì Generated {len(synthetic_data)} synthetic feedback samples\")\n",
    "    \n",
    "    # Train bandit on synthetic data\n",
    "    print(\"\\n[Step 3] Training bandit on synthetic data...\")\n",
    "    for data in synthetic_data:\n",
    "        arm = data['arm_selected']\n",
    "        reward = data['reward']\n",
    "        context = data['user_context']\n",
    "        bandit.update(arm, reward, context)\n",
    "    \n",
    "    print(\"‚úì Training complete\")\n",
    "    \n",
    "    # Display statistics\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"BANDIT STATISTICS AFTER TRAINING\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    stats = bandit.get_statistics()\n",
    "    \n",
    "    print(\"\\nGlobal Performance:\")\n",
    "    print(f\"  Total Selections: {stats['total_selections']}\")\n",
    "    print(f\"  Best Arm: Roadmap Variant {stats['best_arm'] + 1}\")\n",
    "    \n",
    "    print(\"\\n  Q-Values (Estimated Quality):\")\n",
    "    roadmap_names = [\n",
    "        \"Variant 1: Fast-Track\",\n",
    "        \"Variant 2: Balanced\",\n",
    "        \"Variant 3: Self-Paced\",\n",
    "        \"Variant 4: Project-Based\"\n",
    "    ]\n",
    "    for i, (name, q_val) in enumerate(zip(roadmap_names, stats['q_values'])):\n",
    "        print(f\"    {name}: {q_val:.3f}\")\n",
    "    \n",
    "    print(\"\\n  Selection Counts:\")\n",
    "    for i, (name, count) in enumerate(zip(roadmap_names, stats['arm_counts'])):\n",
    "        print(f\"    {name}: {int(count)} times\")\n",
    "    \n",
    "    # Test selections for different user types\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TESTING RECOMMENDATIONS FOR DIFFERENT USERS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    test_users = [\n",
    "        {'experience_level': 'beginner', 'time_commitment': 'part-time'},\n",
    "        {'experience_level': 'intermediate', 'time_commitment': 'full-time'},\n",
    "        {'experience_level': 'advanced', 'time_commitment': 'part-time'},\n",
    "    ]\n",
    "    \n",
    "    for user in test_users:\n",
    "        print(f\"\\nüë§ User: {user['experience_level']}, {user['time_commitment']}\")\n",
    "        \n",
    "        # Get 10 recommendations\n",
    "        recommendations = []\n",
    "        for _ in range(10):\n",
    "            arm = bandit.select_arm(user)\n",
    "            recommendations.append(arm)\n",
    "        \n",
    "        # Count selections\n",
    "        from collections import Counter\n",
    "        counts = Counter(recommendations)\n",
    "        \n",
    "        print(\"  Recommended roadmaps (out of 10 selections):\")\n",
    "        for arm_id, count in sorted(counts.items(), key=lambda x: -x[1]):\n",
    "            print(f\"    {roadmap_names[arm_id]}: {count} times\")\n",
    "    \n",
    "    # Simulate real-time learning\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SIMULATING REAL-TIME LEARNING\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\nSimulating 20 new user interactions...\")\n",
    "    \n",
    "    for i in range(20):\n",
    "        # Random user\n",
    "        user = np.random.choice(test_users)\n",
    "        \n",
    "        # Bandit selects roadmap\n",
    "        selected_arm = bandit.select_arm(user)\n",
    "        \n",
    "        # Simulate user feedback (higher ratings for better matches)\n",
    "        if user['experience_level'] == 'beginner' and selected_arm in [1, 2]:\n",
    "            feedback = np.random.choice([4, 5])\n",
    "        elif user['experience_level'] == 'advanced' and selected_arm == 0:\n",
    "            feedback = 5\n",
    "        elif selected_arm == 3:  # Project-based universally good\n",
    "            feedback = np.random.choice([4, 5])\n",
    "        else:\n",
    "            feedback = np.random.randint(2, 5)\n",
    "        \n",
    "        # Update bandit\n",
    "        bandit.update(selected_arm, feedback, user)\n",
    "        \n",
    "        if (i + 1) % 5 == 0:\n",
    "            print(f\"  Processed {i + 1}/20 interactions...\")\n",
    "    \n",
    "    print(\"\\n‚úì Real-time learning simulation complete\")\n",
    "    \n",
    "    # Show updated statistics\n",
    "    stats_after = bandit.get_statistics()\n",
    "    print(f\"\\n  Total Selections: {stats_after['total_selections']}\")\n",
    "    print(f\"  Best Arm Now: Roadmap Variant {stats_after['best_arm'] + 1}\")\n",
    "    \n",
    "    # Save bandit\n",
    "    print(\"\\n[Step 4] Saving bandit state...\")\n",
    "    bandit.save('contextual_bandit.pkl')\n",
    "    \n",
    "    # Test loading\n",
    "    print(\"\\n[Step 5] Testing load functionality...\")\n",
    "    loaded_bandit = ContextualBandit.load('contextual_bandit.pkl')\n",
    "    print(\"‚úì Bandit loaded successfully\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CONTEXTUAL BANDITS - TEST COMPLETE ‚úì\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\nKey Features:\")\n",
    "    print(\"  ‚úì Contextual awareness (adapts to user type)\")\n",
    "    print(\"  ‚úì Exploration vs Exploitation (epsilon-greedy)\")\n",
    "    print(\"  ‚úì UCB selection strategy\")\n",
    "    print(\"  ‚úì Real-time learning from feedback\")\n",
    "    print(\"  ‚úì Save/Load functionality\")\n",
    "    \n",
    "    print(\"\\nSaved artifacts:\")\n",
    "    print(\"  - contextual_bandit.pkl (trained model)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PathFinder AI - Phase 3: Complete Roadmap System\n",
    "Combines: Gemini Roadmap Generation + Contextual Bandits + Skill Gap Analysis\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "# from phase3_gemini_roadmap_generator import GeminiRoadmapGenerator\n",
    "# from phase3_contextual_bandits import ContextualBandit\n",
    "\n",
    "class RoadmapRecommendationSystem:\n",
    "    \"\"\"Complete roadmap recommendation system with RL\"\"\"\n",
    "    \n",
    "    def __init__(self, gemini_api_key=\"AIzaSyALQl3IlQPXT_dD8k5kvBA9j3aXenmfDAg\", bandit_path='contextual_bandit.pkl'):\n",
    "        \"\"\"\n",
    "        Initialize the system\n",
    "        \n",
    "        Parameters:\n",
    "        gemini_api_key: Gemini API key\n",
    "        bandit_path: path to saved bandit model\n",
    "        \"\"\"\n",
    "        self.roadmap_generator = GeminiRoadmapGenerator(api_key=gemini_api_key)\n",
    "        \n",
    "        # Try to load existing bandit, or create new one\n",
    "        try:\n",
    "            self.bandit = ContextualBandit.load(bandit_path)\n",
    "            print(\"‚úì Loaded existing Contextual Bandit\")\n",
    "        except:\n",
    "            self.bandit = ContextualBandit(n_arms=4, epsilon=0.1)\n",
    "            print(\"‚úì Initialized new Contextual Bandit\")\n",
    "    \n",
    "    def get_personalized_roadmap(self, user_profile):\n",
    "        \"\"\"\n",
    "        Get best roadmap for user using Gemini + RL\n",
    "        \n",
    "        Steps:\n",
    "        1. Generate 4 roadmap variants with Gemini\n",
    "        2. Use Contextual Bandit to select best one\n",
    "        3. Return selected roadmap\n",
    "        \n",
    "        Parameters:\n",
    "        user_profile: dict with user info\n",
    "        \n",
    "        Returns:\n",
    "        dict with selected roadmap and all variants\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"PERSONALIZED ROADMAP RECOMMENDATION\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Step 1: Generate roadmaps with Gemini\n",
    "        print(\"\\n[Step 1] Generating 4 roadmap variants with Gemini...\")\n",
    "        roadmaps = self.roadmap_generator.generate_roadmaps(user_profile)\n",
    "        \n",
    "        if not roadmaps or len(roadmaps) == 0:\n",
    "            print(\"‚ùå Failed to generate roadmaps\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"‚úì Generated {len(roadmaps)} roadmap variants\")\n",
    "        \n",
    "        # Step 2: Contextual Bandit selects best one\n",
    "        print(\"\\n[Step 2] Selecting best roadmap with Contextual Bandit...\")\n",
    "        \n",
    "        user_context = {\n",
    "            'experience_level': user_profile.get('experience_level', 'intermediate'),\n",
    "            'time_commitment': user_profile.get('time_commitment', 'part-time')\n",
    "        }\n",
    "        \n",
    "        selected_arm = self.bandit.select_arm(user_context)\n",
    "        selected_roadmap = roadmaps[selected_arm] if selected_arm < len(roadmaps) else roadmaps[0]\n",
    "        \n",
    "        print(f\"‚úì Selected: {selected_roadmap['roadmap_name']}\")\n",
    "        \n",
    "        return {\n",
    "            'selected_roadmap': selected_roadmap,\n",
    "            'selected_arm': selected_arm,\n",
    "            'all_roadmaps': roadmaps,\n",
    "            'user_context': user_context\n",
    "        }\n",
    "    \n",
    "    def submit_feedback(self, arm_id, rating, user_context):\n",
    "        \"\"\"\n",
    "        User submits feedback on roadmap\n",
    "        \n",
    "        Parameters:\n",
    "        arm_id: which roadmap was shown (0-3)\n",
    "        rating: user rating (1-5 stars or 0/1 for like/dislike)\n",
    "        user_context: user profile\n",
    "        \"\"\"\n",
    "        self.bandit.update(arm_id, rating, user_context)\n",
    "        print(f\"\\n‚úì Feedback recorded: Roadmap {arm_id + 1} rated {rating}/5\")\n",
    "    \n",
    "    def analyze_skill_gap(self, user_skills, required_skills):\n",
    "        \"\"\"Analyze skill gap using Gemini\"\"\"\n",
    "        print(\"\\n[Analyzing Skill Gap]\")\n",
    "        gap_analysis = self.roadmap_generator.analyze_skill_gap(\n",
    "            user_skills, required_skills\n",
    "        )\n",
    "        print(\"‚úì Skill gap analysis complete\")\n",
    "        return gap_analysis\n",
    "    \n",
    "    def get_statistics(self):\n",
    "        \"\"\"Get bandit statistics\"\"\"\n",
    "        return self.bandit.get_statistics()\n",
    "    \n",
    "    def save(self, bandit_path='contextual_bandit.pkl'):\n",
    "        \"\"\"Save bandit state\"\"\"\n",
    "        self.bandit.save(bandit_path)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TESTING\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"PHASE 3: COMPLETE ROADMAP SYSTEM - TEST\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Check for Gemini API key\n",
    "    gemini_key = \"AIzaSyALQl3IlQPXT_dD8k5kvBA9j3aXenmfDAg\"\n",
    "    if not gemini_key:\n",
    "        print(\"\\n‚ö†Ô∏è GEMINI_API_KEY not found\")\n",
    "        print(\"Set it with: export GEMINI_API_KEY='your-key'\")\n",
    "        print(\"\\nThis test requires Gemini API access.\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Initialize system\n",
    "    print(\"\\n[Initializing System]\")\n",
    "    system = RoadmapRecommendationSystem(gemini_api_key=gemini_key)\n",
    "    \n",
    "    # Test Case 1: Junior Developer ‚Üí Senior Developer\n",
    "    print(\"\\n\\n\" + \"=\"*70)\n",
    "    print(\"TEST CASE 1: JUNIOR ‚Üí SENIOR DEVELOPER\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    user_profile_1 = {\n",
    "        'target_career': 'Senior Software Engineer',\n",
    "        'current_skills': ['Python', 'JavaScript', 'React', 'Git', 'HTML', 'CSS'],\n",
    "        'missing_skills': ['System Design', 'Microservices', 'Kubernetes', 'AWS', \n",
    "                          'Team Leadership', 'Code Architecture'],\n",
    "        'experience_level': 'intermediate',\n",
    "        'time_commitment': 'part-time'\n",
    "    }\n",
    "    \n",
    "    result_1 = system.get_personalized_roadmap(user_profile_1)\n",
    "    \n",
    "    if result_1:\n",
    "        selected = result_1['selected_roadmap']\n",
    "        \n",
    "        print(\"\\nüìã SELECTED ROADMAP:\")\n",
    "        print(f\"  Name: {selected['roadmap_name']}\")\n",
    "        print(f\"  Duration: {selected['duration_months']} months\")\n",
    "        print(f\"  Difficulty: {selected['difficulty']}\")\n",
    "        print(f\"  Description: {selected['description']}\")\n",
    "        \n",
    "        print(f\"\\n  Steps ({len(selected['steps'])} total):\")\n",
    "        for step in selected['steps'][:3]:\n",
    "            print(f\"\\n    {step['step_number']}. {step['title']}\")\n",
    "            print(f\"       Duration: {step['duration_weeks']} weeks\")\n",
    "            print(f\"       Skills: {', '.join(step['skills_gained'][:3])}\")\n",
    "        \n",
    "        if len(selected['steps']) > 3:\n",
    "            print(f\"\\n    ... and {len(selected['steps']) - 3} more steps\")\n",
    "        \n",
    "        print(\"\\nüìö ALL AVAILABLE ROADMAPS:\")\n",
    "        for i, roadmap in enumerate(result_1['all_roadmaps'], 1):\n",
    "            print(f\"  {i}. {roadmap['roadmap_name']} ({roadmap['duration_months']} months)\")\n",
    "        \n",
    "        # Simulate user feedback\n",
    "        print(\"\\n[User Feedback] User rates this roadmap...\")\n",
    "        user_rating = 5  # Positive feedback\n",
    "        system.submit_feedback(\n",
    "            arm_id=result_1['selected_arm'],\n",
    "            rating=user_rating,\n",
    "            user_context=result_1['user_context']\n",
    "        )\n",
    "    \n",
    "    # Test Case 2: Career Changer ‚Üí Data Scientist\n",
    "    print(\"\\n\\n\" + \"=\"*70)\n",
    "    print(\"TEST CASE 2: CAREER CHANGE ‚Üí DATA SCIENTIST\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    user_profile_2 = {\n",
    "        'target_career': 'Data Scientist',\n",
    "        'current_skills': ['Excel', 'SQL', 'Statistics'],\n",
    "        'missing_skills': ['Python', 'Machine Learning', 'Deep Learning', 'TensorFlow',\n",
    "                          'Data Visualization', 'Pandas', 'NumPy'],\n",
    "        'experience_level': 'beginner',\n",
    "        'time_commitment': 'full-time'\n",
    "    }\n",
    "    \n",
    "    result_2 = system.get_personalized_roadmap(user_profile_2)\n",
    "    \n",
    "    if result_2:\n",
    "        selected = result_2['selected_roadmap']\n",
    "        \n",
    "        print(\"\\nüìã SELECTED ROADMAP:\")\n",
    "        print(f\"  Name: {selected['roadmap_name']}\")\n",
    "        print(f\"  Duration: {selected['duration_months']} months\")\n",
    "        print(f\"  Description: {selected['description']}\")\n",
    "        \n",
    "        # Simulate feedback\n",
    "        user_rating = 4\n",
    "        system.submit_feedback(\n",
    "            arm_id=result_2['selected_arm'],\n",
    "            rating=user_rating,\n",
    "            user_context=result_2['user_context']\n",
    "        )\n",
    "    \n",
    "    # Test Skill Gap Analysis\n",
    "    print(\"\\n\\n\" + \"=\"*70)\n",
    "    print(\"TEST: SKILL GAP ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    gap_analysis = system.analyze_skill_gap(\n",
    "        user_skills=['Python', 'JavaScript', 'React'],\n",
    "        required_skills=['Python', 'JavaScript', 'React', 'TypeScript', \n",
    "                        'Node.js', 'Docker', 'AWS', 'System Design']\n",
    "    )\n",
    "    \n",
    "    print(\"\\n‚úÖ TRANSFERABLE SKILLS:\")\n",
    "    for skill in gap_analysis.get('transferable_skills', []):\n",
    "        print(f\"  ‚Ä¢ {skill}\")\n",
    "    \n",
    "    print(\"\\nüìö SKILLS TO LEARN:\")\n",
    "    for skill_info in gap_analysis.get('missing_skills', [])[:4]:\n",
    "        print(f\"\\n  ‚Ä¢ {skill_info['skill']}\")\n",
    "        print(f\"    Priority: {skill_info.get('priority', 'N/A')}\")\n",
    "        print(f\"    Time: {skill_info.get('learning_time_weeks', 'N/A')} weeks\")\n",
    "    \n",
    "    # Show Bandit Statistics\n",
    "    print(\"\\n\\n\" + \"=\"*70)\n",
    "    print(\"CONTEXTUAL BANDIT STATISTICS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    stats = system.get_statistics()\n",
    "    \n",
    "    print(\"\\nOverall Performance:\")\n",
    "    print(f\"  Total Recommendations: {stats['total_selections']}\")\n",
    "    print(f\"  Best Performing Roadmap: Variant {stats['best_arm'] + 1}\")\n",
    "    \n",
    "    print(\"\\n  Q-Values (Quality Estimates):\")\n",
    "    for i, q_val in enumerate(stats['q_values'], 1):\n",
    "        print(f\"    Variant {i}: {q_val:.3f}\")\n",
    "    \n",
    "    # Save system\n",
    "    print(\"\\n[Saving System State]\")\n",
    "    system.save('contextual_bandit.pkl')\n",
    "    print(\"‚úì System state saved\")\n",
    "    \n",
    "    # Export results\n",
    "    output_data = {\n",
    "        'test_case_1': {\n",
    "            'user_profile': user_profile_1,\n",
    "            'selected_roadmap': result_1['selected_roadmap'] if result_1 else None\n",
    "        },\n",
    "        'test_case_2': {\n",
    "            'user_profile': user_profile_2,\n",
    "            'selected_roadmap': result_2['selected_roadmap'] if result_2 else None\n",
    "        },\n",
    "        'skill_gap_analysis': gap_analysis,\n",
    "        'bandit_statistics': stats\n",
    "    }\n",
    "    \n",
    "    with open('phase3_test_results.json', 'w') as f:\n",
    "        json.dump(output_data, f, indent=2)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PHASE 3 COMPLETE SYSTEM - TEST COMPLETE ‚úì\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\nComponents Working:\")\n",
    "    print(\"  ‚úì Gemini Roadmap Generation (4 variants)\")\n",
    "    print(\"  ‚úì Contextual Bandits (RL selection)\")\n",
    "    print(\"  ‚úì Skill Gap Analysis\")\n",
    "    print(\"  ‚úì Feedback Learning\")\n",
    "    print(\"  ‚úì Context-Aware Recommendations\")\n",
    "    \n",
    "    print(\"\\nSaved Outputs:\")\n",
    "    print(\"  - phase3_test_results.json\")\n",
    "    print(\"  - contextual_bandit.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PathFinder AI - Phase 3: Gemini Roadmap Generator\n",
    "Generates 3-4 personalized career roadmap variants\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "class GeminiRoadmapGenerator:\n",
    "    \"\"\"Generate career roadmaps using Gemini API\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key=\"AIzaSyALQl3IlQPXT_dD8k5kvBA9j3aXenmfDAg\"):\n",
    "        \"\"\"Initialize Gemini client\"\"\"\n",
    "        if api_key:\n",
    "            self.api_key = api_key\n",
    "        else:\n",
    "            self.api_key = os.environ.get(\"GEMINI_API_KEY\")\n",
    "        \n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"GEMINI_API_KEY not found\")\n",
    "        \n",
    "        self.client = genai.Client(api_key=self.api_key)\n",
    "        self.model = \"gemini-flash-latest\"\n",
    "    \n",
    "    def generate_single_roadmap(self, user_profile, variant_type, roadmap_id):\n",
    "        \"\"\"\n",
    "        Generate a single roadmap variant\n",
    "        \n",
    "        Parameters:\n",
    "        user_profile: dict with user info\n",
    "        variant_type: which variant to generate (1-4)\n",
    "        roadmap_id: ID for this roadmap (1-4)\n",
    "        \n",
    "        Returns:\n",
    "        dict with single roadmap\n",
    "        \"\"\"\n",
    "        \n",
    "        target_career = user_profile.get('target_career', 'Software Engineer')\n",
    "        current_skills = user_profile.get('current_skills', [])\n",
    "        missing_skills = user_profile.get('missing_skills', [])\n",
    "        experience_level = user_profile.get('experience_level', 'intermediate')\n",
    "        time_commitment = user_profile.get('time_commitment', 'part-time')\n",
    "        \n",
    "        # Define variant characteristics\n",
    "        variants = {\n",
    "            1: {\n",
    "                'name': 'Fast-Track Intensive Path',\n",
    "                'description': 'Aggressive timeline with intensive daily study (6-8 months)',\n",
    "                'approach': 'bootcamp-style, intensive learning, rapid skill acquisition'\n",
    "            },\n",
    "            2: {\n",
    "                'name': 'Balanced Structured Path',\n",
    "                'description': 'Moderate pace with structured curriculum (9-12 months)',\n",
    "                'approach': 'balanced learning, structured progression, steady growth'\n",
    "            },\n",
    "            3: {\n",
    "                'name': 'Self-Paced Flexible Path',\n",
    "                'description': 'Relaxed timeline with flexible scheduling (12-18 months)',\n",
    "                'approach': 'flexible learning, self-paced study, adaptable schedule'\n",
    "            },\n",
    "            4: {\n",
    "                'name': 'Project-Based Practical Path',\n",
    "                'description': 'Learn by building real projects (10-12 months)',\n",
    "                'approach': 'hands-on projects, practical application, learning by doing'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        variant = variants[variant_type]\n",
    "        \n",
    "        prompt = f\"\"\"You are a career roadmap expert. Generate ONE detailed learning roadmap for becoming a {target_career}.\n",
    "\n",
    "USER PROFILE:\n",
    "- Target Career: {target_career}\n",
    "- Current Skills: {', '.join(current_skills) if current_skills else 'None'}\n",
    "- Skills to Learn: {', '.join(missing_skills) if missing_skills else 'Various'}\n",
    "- Experience Level: {experience_level}\n",
    "- Time Commitment: {time_commitment}\n",
    "\n",
    "ROADMAP TYPE: {variant['name']}\n",
    "Approach: {variant['approach']}\n",
    "Description: {variant['description']}\n",
    "\n",
    "REQUIREMENTS:\n",
    "1. Create a roadmap with:\n",
    "   - roadmap_id: {roadmap_id}\n",
    "   - roadmap_name: \"{variant['name']}\"\n",
    "   - description: 1-2 sentences describing this specific path\n",
    "   - duration_months: realistic time estimate\n",
    "   - difficulty: \"beginner\", \"intermediate\", or \"advanced\"\n",
    "   - steps: array of 5-6 learning steps (keep it concise)\n",
    "\n",
    "2. Each step must have:\n",
    "   - step_number: sequential number\n",
    "   - title: clear, specific step name\n",
    "   - description: brief 1-sentence what to learn\n",
    "   - duration_weeks: time for this step\n",
    "   - resources: 2 specific learning resources\n",
    "   - skills_gained: 2-4 skills learned\n",
    "\n",
    "OUTPUT FORMAT (JSON only, no markdown):\n",
    "{{\n",
    "  \"roadmap_id\": {roadmap_id},\n",
    "  \"roadmap_name\": \"{variant['name']}\",\n",
    "  \"description\": \"brief description here\",\n",
    "  \"duration_months\": 6,\n",
    "  \"difficulty\": \"intermediate\",\n",
    "  \"steps\": [\n",
    "    {{\n",
    "      \"step_number\": 1,\n",
    "      \"title\": \"Step title\",\n",
    "      \"description\": \"Brief description\",\n",
    "      \"duration_weeks\": 3,\n",
    "      \"resources\": [\"Resource 1\", \"Resource 2\"],\n",
    "      \"skills_gained\": [\"Skill1\", \"Skill2\"]\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "Return ONLY valid JSON.\"\"\"\n",
    "\n",
    "        try:\n",
    "            contents = [\n",
    "                types.Content(\n",
    "                    role=\"user\",\n",
    "                    parts=[types.Part.from_text(text=prompt)],\n",
    "                ),\n",
    "            ]\n",
    "            \n",
    "            generate_content_config = types.GenerateContentConfig(\n",
    "                temperature=0.7,\n",
    "                top_p=0.95,\n",
    "                top_k=40\n",
    "            )\n",
    "            \n",
    "            response = self.client.models.generate_content(\n",
    "                model=self.model,\n",
    "                contents=contents,\n",
    "                config=generate_content_config,\n",
    "            )\n",
    "            \n",
    "            response_text = response.text.strip()\n",
    "            \n",
    "            # Clean markdown\n",
    "            if response_text.startswith(\"```json\"):\n",
    "                response_text = response_text.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "            elif response_text.startswith(\"```\"):\n",
    "                response_text = response_text.replace(\"```\", \"\").strip()\n",
    "            \n",
    "            # Parse JSON\n",
    "            roadmap = json.loads(response_text)\n",
    "            \n",
    "            return roadmap\n",
    "        \n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Warning: Failed to parse roadmap {roadmap_id}: {e}\")\n",
    "            return None\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error generating roadmap {roadmap_id}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def generate_roadmaps(self, user_profile):\n",
    "        \"\"\"\n",
    "        Generate 4 roadmap variants by calling API 4 times\n",
    "        \n",
    "        Parameters:\n",
    "        user_profile: dict containing user info\n",
    "        \n",
    "        Returns:\n",
    "        list of 4 roadmap variants\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"Generating 4 roadmap variants (this takes ~20-30 seconds)...\")\n",
    "        \n",
    "        roadmaps = []\n",
    "        \n",
    "        for variant_id in range(1, 5):\n",
    "            print(f\"  Generating Variant {variant_id}/4...\", end=\" \")\n",
    "            \n",
    "            roadmap = self.generate_single_roadmap(user_profile, variant_id, variant_id)\n",
    "            \n",
    "            if roadmap:\n",
    "                roadmaps.append(roadmap)\n",
    "                print(\"‚úì\")\n",
    "            else:\n",
    "                print(\"‚úó (failed)\")\n",
    "        \n",
    "        print(f\"Successfully generated {len(roadmaps)}/4 roadmaps\")\n",
    "        \n",
    "        return roadmaps\n",
    "    \n",
    "    def analyze_skill_gap(self, user_skills, required_skills):\n",
    "        \"\"\"\n",
    "        Analyze skill gap between current and required skills\n",
    "        \n",
    "        Parameters:\n",
    "        user_skills: list of current skills\n",
    "        required_skills: list of required skills for career\n",
    "        \n",
    "        Returns:\n",
    "        dict with gap analysis\n",
    "        \"\"\"\n",
    "        \n",
    "        prompt = f\"\"\"Analyze the skill gap for career transition.\n",
    "\n",
    "CURRENT SKILLS:\n",
    "{', '.join(user_skills) if user_skills else 'None'}\n",
    "\n",
    "REQUIRED SKILLS:\n",
    "{', '.join(required_skills)}\n",
    "\n",
    "TASK:\n",
    "1. Identify which current skills are transferable\n",
    "2. List skills that need to be learned\n",
    "3. Prioritize missing skills (high/medium/low priority)\n",
    "4. Estimate learning time for each missing skill\n",
    "\n",
    "OUTPUT FORMAT (JSON only):\n",
    "{{\n",
    "  \"transferable_skills\": [\"skill1\", \"skill2\"],\n",
    "  \"missing_skills\": [\n",
    "    {{\n",
    "      \"skill\": \"Machine Learning\",\n",
    "      \"priority\": \"high\",\n",
    "      \"learning_time_weeks\": 8,\n",
    "      \"difficulty\": \"intermediate\",\n",
    "      \"reason\": \"Core requirement for data science roles\"\n",
    "    }},\n",
    "    ...\n",
    "  ],\n",
    "  \"learning_path_summary\": \"Brief 2-3 sentence summary of recommended approach\"\n",
    "}}\n",
    "\n",
    "Return ONLY valid JSON.\"\"\"\n",
    "\n",
    "        try:\n",
    "            contents = [\n",
    "                types.Content(\n",
    "                    role=\"user\",\n",
    "                    parts=[types.Part.from_text(text=prompt)],\n",
    "                ),\n",
    "            ]\n",
    "            \n",
    "            generate_content_config = types.GenerateContentConfig(\n",
    "                temperature=0.3\n",
    "            )\n",
    "            \n",
    "            response = self.client.models.generate_content(\n",
    "                model=self.model,\n",
    "                contents=contents,\n",
    "                config=generate_content_config,\n",
    "            )\n",
    "            \n",
    "            response_text = response.text.strip()\n",
    "            \n",
    "            if response_text.startswith(\"```json\"):\n",
    "                response_text = response_text.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "            elif response_text.startswith(\"```\"):\n",
    "                response_text = response_text.replace(\"```\", \"\").strip()\n",
    "            \n",
    "            gap_analysis = json.loads(response_text)\n",
    "            \n",
    "            return gap_analysis\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error in skill gap analysis: {e}\")\n",
    "            return {\n",
    "                \"transferable_skills\": [],\n",
    "                \"missing_skills\": [],\n",
    "                \"learning_path_summary\": \"Unable to analyze skill gap\"\n",
    "            }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TESTING\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"PHASE 3: GEMINI ROADMAP GENERATOR - TEST\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    try:\n",
    "        generator = GeminiRoadmapGenerator()\n",
    "        \n",
    "        # Test Case 1: Data Scientist Career\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"TEST 1: DATA SCIENTIST ROADMAP\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        user_profile_1 = {\n",
    "            'target_career': 'Data Scientist',\n",
    "            'current_skills': ['Python', 'SQL', 'Excel'],\n",
    "            'missing_skills': ['Machine Learning', 'Statistics', 'Deep Learning', 'TensorFlow'],\n",
    "            'experience_level': 'beginner',\n",
    "            'time_commitment': 'part-time'\n",
    "        }\n",
    "        \n",
    "        print(\"\\nGenerating roadmaps (one-by-one for reliability)...\")\n",
    "        roadmaps_1 = generator.generate_roadmaps(user_profile_1)\n",
    "        \n",
    "        if len(roadmaps_1) == 0:\n",
    "            print(\"\\n‚ùå Failed to generate any roadmaps. Check API key and network.\")\n",
    "            exit(1)\n",
    "        \n",
    "        print(f\"\\n‚úì Successfully generated {len(roadmaps_1)} roadmap variants\\n\")\n",
    "        \n",
    "        for roadmap in roadmaps_1:\n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(f\"ROADMAP {roadmap['roadmap_id']}: {roadmap['roadmap_name']}\")\n",
    "            print(f\"{'='*70}\")\n",
    "            print(f\"Description: {roadmap['description']}\")\n",
    "            print(f\"Duration: {roadmap['duration_months']} months\")\n",
    "            print(f\"Difficulty: {roadmap['difficulty']}\")\n",
    "            print(f\"\\nSteps ({len(roadmap['steps'])} total):\")\n",
    "            \n",
    "            for step in roadmap['steps'][:3]:  # Show first 3 steps\n",
    "                print(f\"\\n  Step {step['step_number']}: {step['title']}\")\n",
    "                print(f\"  Duration: {step['duration_weeks']} weeks\")\n",
    "                print(f\"  Skills: {', '.join(step['skills_gained'])}\")\n",
    "                print(f\"  Resources: {', '.join(step['resources'][:2])}\")\n",
    "            \n",
    "            if len(roadmap['steps']) > 3:\n",
    "                print(f\"\\n  ... and {len(roadmap['steps']) - 3} more steps\")\n",
    "        \n",
    "        # Test Case 2: Skill Gap Analysis\n",
    "        print(\"\\n\\n\" + \"=\"*70)\n",
    "        print(\"TEST 2: SKILL GAP ANALYSIS\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        print(\"\\nAnalyzing skill gap...\")\n",
    "        gap_analysis = generator.analyze_skill_gap(\n",
    "            user_skills=['Python', 'SQL', 'Excel'],\n",
    "            required_skills=['Python', 'SQL', 'Machine Learning', 'Statistics', \n",
    "                           'Deep Learning', 'TensorFlow', 'Data Visualization']\n",
    "        )\n",
    "        \n",
    "        print(\"\\n--- SKILL GAP ANALYSIS ---\\n\")\n",
    "        \n",
    "        print(\"‚úì TRANSFERABLE SKILLS:\")\n",
    "        for skill in gap_analysis.get('transferable_skills', []):\n",
    "            print(f\"  ‚Ä¢ {skill}\")\n",
    "        \n",
    "        print(\"\\nüìö SKILLS TO LEARN:\")\n",
    "        for skill_info in gap_analysis.get('missing_skills', [])[:5]:\n",
    "            print(f\"\\n  ‚Ä¢ {skill_info['skill']}\")\n",
    "            print(f\"    Priority: {skill_info['priority']}\")\n",
    "            print(f\"    Time: {skill_info['learning_time_weeks']} weeks\")\n",
    "            print(f\"    Difficulty: {skill_info['difficulty']}\")\n",
    "        \n",
    "        print(f\"\\nüí° SUMMARY:\")\n",
    "        print(f\"  {gap_analysis.get('learning_path_summary', 'N/A')}\")\n",
    "        \n",
    "        # Save roadmaps to JSON\n",
    "        output_data = {\n",
    "            'user_profile': user_profile_1,\n",
    "            'roadmaps': roadmaps_1,\n",
    "            'skill_gap_analysis': gap_analysis\n",
    "        }\n",
    "        \n",
    "        with open('generated_roadmaps.json', 'w') as f:\n",
    "            json.dump(output_data, f, indent=2)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"GEMINI ROADMAP GENERATOR - TEST COMPLETE ‚úì\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"\\nOutputs:\")\n",
    "        print(\"  ‚úì 4 roadmap variants generated\")\n",
    "        print(\"  ‚úì Skill gap analysis complete\")\n",
    "        print(\"  ‚úì Results saved to generated_roadmaps.json\")\n",
    "        \n",
    "    except ValueError as e:\n",
    "        print(f\"\\n‚ùå ERROR: {e}\")\n",
    "        print(\"\\nTo fix: Set GEMINI_API_KEY environment variable\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Unexpected error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PathFinder AI - Phase 4: Policy Gradient (RL Algorithm #2)\n",
    "Suggests optimal next step in a learning roadmap based on user progress\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import json\n",
    "import pickle\n",
    "from collections import deque\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"Neural network for policy gradient\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        \"\"\"\n",
    "        Initialize policy network\n",
    "        \n",
    "        Parameters:\n",
    "        state_dim: dimension of state vector\n",
    "        action_dim: number of possible actions (steps in roadmap)\n",
    "        hidden_dim: size of hidden layers\n",
    "        \"\"\"\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        \"\"\"Forward pass through network\"\"\"\n",
    "        return self.network(state)\n",
    "\n",
    "\n",
    "class RoadmapPolicyGradient:\n",
    "    \"\"\"\n",
    "    Policy Gradient agent for roadmap step recommendations\n",
    "    Learns optimal sequence of learning steps\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim=20, max_roadmap_steps=10, learning_rate=0.001):\n",
    "        \"\"\"\n",
    "        Initialize Policy Gradient agent\n",
    "        \n",
    "        Parameters:\n",
    "        state_dim: size of state representation\n",
    "        max_roadmap_steps: maximum steps in a roadmap\n",
    "        learning_rate: learning rate for optimizer\n",
    "        \"\"\"\n",
    "        self.state_dim = state_dim\n",
    "        self.max_steps = max_roadmap_steps\n",
    "        self.action_dim = max_roadmap_steps  # Each step is an action\n",
    "        \n",
    "        # Policy network\n",
    "        self.policy_net = PolicyNetwork(state_dim, self.action_dim).to(device)\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # Training memory\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "        self.gamma = 0.99  # Discount factor\n",
    "        \n",
    "        # Statistics\n",
    "        self.episode_rewards = []\n",
    "        self.episode_count = 0\n",
    "    \n",
    "    def create_state(self, user_progress):\n",
    "        \"\"\"\n",
    "        Create state vector from user progress\n",
    "        \n",
    "        Parameters:\n",
    "        user_progress: dict with:\n",
    "            - completed_steps: list of completed step indices\n",
    "            - current_skills: list of acquired skills\n",
    "            - time_spent_weeks: total time invested\n",
    "            - engagement_score: 0-1 (based on consistency)\n",
    "            - difficulty_preference: 0-1 (easy to hard)\n",
    "        \n",
    "        Returns:\n",
    "        state vector (numpy array)\n",
    "        \"\"\"\n",
    "        # Initialize state vector\n",
    "        state = np.zeros(self.state_dim)\n",
    "        \n",
    "        # Feature 1-10: Which steps completed (one-hot encoded)\n",
    "        completed = user_progress.get('completed_steps', [])\n",
    "        for step_idx in completed:\n",
    "            if step_idx < 10:\n",
    "                state[step_idx] = 1.0\n",
    "        \n",
    "        # Feature 11: Progress percentage\n",
    "        state[10] = len(completed) / self.max_steps if self.max_steps > 0 else 0\n",
    "        \n",
    "        # Feature 12: Skills acquired count (normalized)\n",
    "        current_skills = user_progress.get('current_skills', [])\n",
    "        state[11] = len(current_skills) / 20.0  # Normalize assuming max 20 skills\n",
    "        \n",
    "        # Feature 13: Time spent (normalized to weeks)\n",
    "        time_spent = user_progress.get('time_spent_weeks', 0)\n",
    "        state[12] = min(time_spent / 52.0, 1.0)  # Normalize to 1 year max\n",
    "        \n",
    "        # Feature 14: Engagement score\n",
    "        state[13] = user_progress.get('engagement_score', 0.5)\n",
    "        \n",
    "        # Feature 14: Difficulty preference\n",
    "        state[14] = user_progress.get('difficulty_preference', 0.5)\n",
    "        \n",
    "        # Feature 15-19: Recent performance (last 5 steps)\n",
    "        recent_performance = user_progress.get('recent_performance', [])\n",
    "        for i, perf in enumerate(recent_performance[-5:]):\n",
    "            if i < 5:\n",
    "                state[15 + i] = perf\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        \"\"\"\n",
    "        Select next step to recommend\n",
    "        \n",
    "        Parameters:\n",
    "        state: numpy array representing current state\n",
    "        \n",
    "        Returns:\n",
    "        action (step index), log_prob\n",
    "        \"\"\"\n",
    "        # Convert to tensor\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Get action probabilities\n",
    "        probs = self.policy_net(state_tensor)\n",
    "        \n",
    "        # Sample action from distribution\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        \n",
    "        # Save log probability for training\n",
    "        self.saved_log_probs.append(m.log_prob(action))\n",
    "        \n",
    "        return action.item(), m.log_prob(action).item()\n",
    "    \n",
    "    def store_reward(self, reward):\n",
    "        \"\"\"Store reward for current step\"\"\"\n",
    "        self.rewards.append(reward)\n",
    "    \n",
    "    def calculate_returns(self):\n",
    "        \"\"\"Calculate discounted returns\"\"\"\n",
    "        returns = []\n",
    "        R = 0\n",
    "        \n",
    "        # Calculate returns backwards\n",
    "        for r in reversed(self.rewards):\n",
    "            R = r + self.gamma * R\n",
    "            returns.insert(0, R)\n",
    "        \n",
    "        # Normalize returns\n",
    "        returns = torch.tensor(returns).to(device)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "        \n",
    "        return returns\n",
    "    \n",
    "    def update_policy(self):\n",
    "        \"\"\"Update policy using REINFORCE algorithm\"\"\"\n",
    "        if len(self.rewards) == 0:\n",
    "            return 0\n",
    "        \n",
    "        # Calculate returns\n",
    "        returns = self.calculate_returns()\n",
    "        \n",
    "        # Calculate policy loss\n",
    "        policy_loss = []\n",
    "        for log_prob, R in zip(self.saved_log_probs, returns):\n",
    "            policy_loss.append(-log_prob * R)\n",
    "        \n",
    "        # Update policy network\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = torch.stack(policy_loss).sum()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Clear memory\n",
    "        loss_value = loss.item()\n",
    "        del self.saved_log_probs[:]\n",
    "        del self.rewards[:]\n",
    "        \n",
    "        return loss_value\n",
    "    \n",
    "    def train_episode(self, episode_data):\n",
    "        \"\"\"\n",
    "        Train on one complete episode (user journey)\n",
    "        \n",
    "        Parameters:\n",
    "        episode_data: list of (state, action, reward) tuples\n",
    "        \n",
    "        Returns:\n",
    "        total reward, loss\n",
    "        \"\"\"\n",
    "        total_reward = 0\n",
    "        \n",
    "        for state, action, reward in episode_data:\n",
    "            # Store state-action-reward\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            probs = self.policy_net(state_tensor)\n",
    "            m = Categorical(probs)\n",
    "            \n",
    "            # Recreate action tensor\n",
    "            action_tensor = torch.tensor([action]).to(device)\n",
    "            log_prob = m.log_prob(action_tensor)\n",
    "            \n",
    "            self.saved_log_probs.append(log_prob)\n",
    "            self.rewards.append(reward)\n",
    "            total_reward += reward\n",
    "        \n",
    "        # Update policy\n",
    "        loss = self.update_policy()\n",
    "        \n",
    "        # Track statistics\n",
    "        self.episode_rewards.append(total_reward)\n",
    "        self.episode_count += 1\n",
    "        \n",
    "        return total_reward, loss\n",
    "    \n",
    "    def recommend_next_step(self, user_progress, roadmap_steps, completed_steps):\n",
    "        \"\"\"\n",
    "        Recommend next step in roadmap\n",
    "        \n",
    "        Parameters:\n",
    "        user_progress: dict with user info\n",
    "        roadmap_steps: list of all steps in roadmap\n",
    "        completed_steps: list of completed step indices\n",
    "        \n",
    "        Returns:\n",
    "        recommended step dict, confidence score\n",
    "        \"\"\"\n",
    "        # Create state\n",
    "        state = self.create_state(user_progress)\n",
    "        \n",
    "        # Get action probabilities\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            probs = self.policy_net(state_tensor).cpu().numpy()[0]\n",
    "        \n",
    "        # Mask completed steps\n",
    "        available_steps = [i for i in range(len(roadmap_steps)) if i not in completed_steps]\n",
    "        \n",
    "        if not available_steps:\n",
    "            return None, 0.0\n",
    "        \n",
    "        # Get probabilities for available steps\n",
    "        available_probs = {i: probs[i] for i in available_steps}\n",
    "        \n",
    "        # Select step with highest probability\n",
    "        best_step_idx = max(available_probs, key=available_probs.get)\n",
    "        confidence = available_probs[best_step_idx]\n",
    "        \n",
    "        # Get step details\n",
    "        recommended_step = roadmap_steps[best_step_idx]\n",
    "        \n",
    "        return recommended_step, float(confidence)\n",
    "    \n",
    "    def save(self, filepath='policy_gradient_model.pkl'):\n",
    "        \"\"\"Save model\"\"\"\n",
    "        torch.save({\n",
    "            'policy_net_state': self.policy_net.state_dict(),\n",
    "            'optimizer_state': self.optimizer.state_dict(),\n",
    "            'episode_rewards': self.episode_rewards,\n",
    "            'episode_count': self.episode_count,\n",
    "            'state_dim': self.state_dim,\n",
    "            'max_steps': self.max_steps,\n",
    "            'action_dim': self.action_dim\n",
    "        }, filepath)\n",
    "        print(f\"Policy Gradient model saved to {filepath}\")\n",
    "    \n",
    "    def load(self, filepath='policy_gradient_model.pkl'):\n",
    "        \"\"\"Load model\"\"\"\n",
    "        checkpoint = torch.load(filepath, map_location=device)\n",
    "        self.policy_net.load_state_dict(checkpoint['policy_net_state'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
    "        self.episode_rewards = checkpoint['episode_rewards']\n",
    "        self.episode_count = checkpoint['episode_count']\n",
    "        print(f\"Policy Gradient model loaded from {filepath}\")\n",
    "    \n",
    "    def get_statistics(self):\n",
    "        \"\"\"Get training statistics\"\"\"\n",
    "        if len(self.episode_rewards) == 0:\n",
    "            return {\n",
    "                'total_episodes': 0,\n",
    "                'average_reward': 0,\n",
    "                'recent_rewards': []\n",
    "            }\n",
    "        \n",
    "        return {\n",
    "            'total_episodes': self.episode_count,\n",
    "            'average_reward': np.mean(self.episode_rewards),\n",
    "            'recent_rewards': self.episode_rewards[-10:],\n",
    "            'total_reward': sum(self.episode_rewards)\n",
    "        }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SYNTHETIC DATA GENERATOR\n",
    "# ============================================================================\n",
    "\n",
    "def generate_synthetic_episode(num_steps=6):\n",
    "    \"\"\"\n",
    "    Generate synthetic user journey for training\n",
    "    \n",
    "    Simulates: User follows roadmap ‚Üí completes steps ‚Üí gets rewards\n",
    "    \n",
    "    Returns:\n",
    "    list of (state, action, reward) tuples\n",
    "    \"\"\"\n",
    "    episode = []\n",
    "    completed_steps = []\n",
    "    skills_acquired = []\n",
    "    \n",
    "    for step_idx in range(num_steps):\n",
    "        # Create user progress state\n",
    "        user_progress = {\n",
    "            'completed_steps': completed_steps.copy(),\n",
    "            'current_skills': skills_acquired.copy(),\n",
    "            'time_spent_weeks': step_idx * 2,  # 2 weeks per step\n",
    "            'engagement_score': np.random.uniform(0.6, 1.0),\n",
    "            'difficulty_preference': np.random.uniform(0.3, 0.8),\n",
    "            'recent_performance': [np.random.uniform(0.6, 1.0) for _ in range(min(step_idx, 5))]\n",
    "        }\n",
    "        \n",
    "        # Create state vector\n",
    "        agent = RoadmapPolicyGradient()\n",
    "        state = agent.create_state(user_progress)\n",
    "        \n",
    "        # Action: recommend next step (sequential for now)\n",
    "        action = step_idx\n",
    "        \n",
    "        # Reward: higher if user completes step successfully\n",
    "        if np.random.random() < 0.8:  # 80% completion rate\n",
    "            reward = 1.0  # Completed\n",
    "            completed_steps.append(step_idx)\n",
    "            skills_acquired.extend([f\"skill_{step_idx}\"])\n",
    "        else:\n",
    "            reward = -0.5  # Skipped/failed\n",
    "        \n",
    "        episode.append((state, action, reward))\n",
    "    \n",
    "    return episode\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TESTING\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"PHASE 4: POLICY GRADIENT - TEST\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Initialize agent\n",
    "    print(\"\\n[Step 1] Initializing Policy Gradient agent...\")\n",
    "    agent = RoadmapPolicyGradient(state_dim=20, max_roadmap_steps=10)\n",
    "    print(f\"‚úì Agent initialized\")\n",
    "    print(f\"  Device: {device}\")\n",
    "    print(f\"  State dimension: {agent.state_dim}\")\n",
    "    print(f\"  Action dimension: {agent.action_dim}\")\n",
    "    \n",
    "    # Generate synthetic training data\n",
    "    print(\"\\n[Step 2] Generating synthetic training data...\")\n",
    "    num_episodes = 100\n",
    "    synthetic_episodes = [generate_synthetic_episode(num_steps=6) for _ in range(num_episodes)]\n",
    "    print(f\"‚úì Generated {num_episodes} synthetic episodes\")\n",
    "    \n",
    "    # Train agent\n",
    "    print(\"\\n[Step 3] Training Policy Gradient agent...\")\n",
    "    print(\"This may take 1-2 minutes...\")\n",
    "    \n",
    "    for episode_idx, episode_data in enumerate(synthetic_episodes):\n",
    "        total_reward, loss = agent.train_episode(episode_data)\n",
    "        \n",
    "        if (episode_idx + 1) % 20 == 0:\n",
    "            avg_reward = np.mean(agent.episode_rewards[-20:])\n",
    "            print(f\"  Episode {episode_idx + 1}/{num_episodes} | Avg Reward: {avg_reward:.2f} | Loss: {loss:.4f}\")\n",
    "    \n",
    "    print(\"‚úì Training complete\")\n",
    "    \n",
    "    # Display statistics\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TRAINING STATISTICS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    stats = agent.get_statistics()\n",
    "    print(f\"\\nTotal Episodes: {stats['total_episodes']}\")\n",
    "    print(f\"Average Reward: {stats['average_reward']:.2f}\")\n",
    "    print(f\"Total Reward: {stats['total_reward']:.2f}\")\n",
    "    print(f\"\\nRecent Rewards (last 10 episodes):\")\n",
    "    for i, reward in enumerate(stats['recent_rewards'], 1):\n",
    "        print(f\"  Episode {stats['total_episodes'] - 10 + i}: {reward:.2f}\")\n",
    "    \n",
    "    # Test recommendation\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TEST: NEXT STEP RECOMMENDATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Mock roadmap\n",
    "    mock_roadmap = [\n",
    "        {'step_number': 1, 'title': 'Python Fundamentals', 'duration_weeks': 3},\n",
    "        {'step_number': 2, 'title': 'Data Structures', 'duration_weeks': 4},\n",
    "        {'step_number': 3, 'title': 'Web Frameworks', 'duration_weeks': 3},\n",
    "        {'step_number': 4, 'title': 'Database Design', 'duration_weeks': 3},\n",
    "        {'step_number': 5, 'title': 'API Development', 'duration_weeks': 4},\n",
    "        {'step_number': 6, 'title': 'Deployment', 'duration_weeks': 2},\n",
    "    ]\n",
    "    \n",
    "    # Test user who completed first 2 steps\n",
    "    test_user_progress = {\n",
    "        'completed_steps': [0, 1],  # Completed steps 1 and 2\n",
    "        'current_skills': ['Python', 'Data Structures', 'Algorithms'],\n",
    "        'time_spent_weeks': 7,\n",
    "        'engagement_score': 0.85,\n",
    "        'difficulty_preference': 0.6,\n",
    "        'recent_performance': [0.9, 0.8]\n",
    "    }\n",
    "    \n",
    "    print(\"\\nTest User Profile:\")\n",
    "    print(f\"  Completed Steps: {len(test_user_progress['completed_steps'])}/6\")\n",
    "    print(f\"  Current Skills: {', '.join(test_user_progress['current_skills'])}\")\n",
    "    print(f\"  Time Spent: {test_user_progress['time_spent_weeks']} weeks\")\n",
    "    print(f\"  Engagement: {test_user_progress['engagement_score']:.2f}\")\n",
    "    \n",
    "    # Get recommendation\n",
    "    recommended_step, confidence = agent.recommend_next_step(\n",
    "        test_user_progress,\n",
    "        mock_roadmap,\n",
    "        test_user_progress['completed_steps']\n",
    "    )\n",
    "    \n",
    "    if recommended_step:\n",
    "        print(f\"\\n‚úì RECOMMENDED NEXT STEP:\")\n",
    "        print(f\"  Step {recommended_step['step_number']}: {recommended_step['title']}\")\n",
    "        print(f\"  Duration: {recommended_step['duration_weeks']} weeks\")\n",
    "        print(f\"  Confidence: {confidence * 100:.1f}%\")\n",
    "    \n",
    "    # Save model\n",
    "    print(\"\\n[Step 4] Saving model...\")\n",
    "    agent.save('policy_gradient_model.pkl')\n",
    "    \n",
    "    # Test loading\n",
    "    print(\"\\n[Step 5] Testing model loading...\")\n",
    "    new_agent = RoadmapPolicyGradient()\n",
    "    new_agent.load('policy_gradient_model.pkl')\n",
    "    print(\"‚úì Model loaded successfully\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"POLICY GRADIENT - TEST COMPLETE ‚úì\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\nKey Features:\")\n",
    "    print(\"  ‚úì Neural network policy\")\n",
    "    print(\"  ‚úì REINFORCE algorithm\")\n",
    "    print(\"  ‚úì Synthetic data training\")\n",
    "    print(\"  ‚úì Next step recommendations\")\n",
    "    print(\"  ‚úì Confidence scores\")\n",
    "    print(\"  ‚úì Save/Load functionality\")\n",
    "    \n",
    "    print(\"\\nSaved artifacts:\")\n",
    "    print(\"  - policy_gradient_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PathFinder AI - Phase 4: Career Guidance Chatbot\n",
    "Gemini-powered chatbot for answering career questions\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from collections import deque\n",
    "\n",
    "class CareerGuidanceChatbot:\n",
    "    \"\"\"Conversational AI for career guidance using Gemini\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key=None):\n",
    "        \"\"\"Initialize chatbot with Gemini API\"\"\"\n",
    "        if api_key:\n",
    "            self.api_key = api_key\n",
    "        else:\n",
    "            self.api_key = os.environ.get(\"GEMINI_API_KEY\")\n",
    "        \n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"GEMINI_API_KEY not found\")\n",
    "        \n",
    "        self.client = genai.Client(api_key=self.api_key)\n",
    "        self.model = \"gemini-flash-latest\"\n",
    "        \n",
    "        # Conversation history (last 10 messages)\n",
    "        self.conversation_history = deque(maxlen=10)\n",
    "        \n",
    "        # System context\n",
    "        self.system_context = \"\"\"You are PathFinder AI, an expert career guidance counselor and educational advisor.\n",
    "\n",
    "Your role:\n",
    "- Help users with career decisions and planning\n",
    "- Provide advice on skill development and learning paths\n",
    "- Answer questions about job markets and industries\n",
    "- Guide users through career transitions\n",
    "- Offer encouragement and motivation\n",
    "\n",
    "Your personality:\n",
    "- Professional but friendly and approachable\n",
    "- Supportive and encouraging\n",
    "- Practical and action-oriented\n",
    "- Honest about challenges while staying positive\n",
    "\n",
    "Guidelines:\n",
    "- Keep responses concise (2-4 sentences for simple questions, longer for complex ones)\n",
    "- Provide specific, actionable advice when possible\n",
    "- If you don't know something, say so honestly\n",
    "- Ask clarifying questions when needed\n",
    "- Reference the user's profile/progress when relevant\"\"\"\n",
    "    \n",
    "    def set_user_context(self, user_profile):\n",
    "        \"\"\"\n",
    "        Set context about current user\n",
    "        \n",
    "        Parameters:\n",
    "        user_profile: dict with:\n",
    "            - name: user's name\n",
    "            - target_career: desired career\n",
    "            - current_skills: list of skills\n",
    "            - completed_steps: roadmap progress\n",
    "            - experience_level: beginner/intermediate/advanced\n",
    "        \"\"\"\n",
    "        self.user_context = user_profile\n",
    "    \n",
    "    def chat(self, user_message, use_context=True):\n",
    "        \"\"\"\n",
    "        Send message to chatbot and get response\n",
    "        \n",
    "        Parameters:\n",
    "        user_message: string from user\n",
    "        use_context: whether to include user profile context\n",
    "        \n",
    "        Returns:\n",
    "        bot response string\n",
    "        \"\"\"\n",
    "        \n",
    "        # Build conversation context\n",
    "        messages = []\n",
    "        \n",
    "        # Add system context\n",
    "        context_message = self.system_context\n",
    "        \n",
    "        # Add user context if available and requested\n",
    "        if use_context and hasattr(self, 'user_context'):\n",
    "            uc = self.user_context\n",
    "            context_message += f\"\\n\\nCurrent User Profile:\"\n",
    "            context_message += f\"\\n- Name: {uc.get('name', 'User')}\"\n",
    "            context_message += f\"\\n- Target Career: {uc.get('target_career', 'Not specified')}\"\n",
    "            context_message += f\"\\n- Current Skills: {', '.join(uc.get('current_skills', [])[:5])}\"\n",
    "            context_message += f\"\\n- Experience Level: {uc.get('experience_level', 'Not specified')}\"\n",
    "            \n",
    "            if uc.get('completed_steps'):\n",
    "                context_message += f\"\\n- Roadmap Progress: {len(uc.get('completed_steps', []))} steps completed\"\n",
    "        \n",
    "        # Add conversation history\n",
    "        for role, content in self.conversation_history:\n",
    "            messages.append(types.Content(\n",
    "                role=role,\n",
    "                parts=[types.Part.from_text(text=content)]\n",
    "            ))\n",
    "        \n",
    "        # Add current user message with context\n",
    "        full_user_message = f\"{context_message}\\n\\nUser Question: {user_message}\" if len(messages) == 0 else user_message\n",
    "        \n",
    "        messages.append(types.Content(\n",
    "            role=\"user\",\n",
    "            parts=[types.Part.from_text(text=full_user_message)]\n",
    "        ))\n",
    "        \n",
    "        try:\n",
    "            generate_content_config = types.GenerateContentConfig(\n",
    "                temperature=0.7,  # Balanced creativity\n",
    "                top_p=0.95,\n",
    "                top_k=40\n",
    "            )\n",
    "            \n",
    "            response = self.client.models.generate_content(\n",
    "                model=self.model,\n",
    "                contents=messages,\n",
    "                config=generate_content_config,\n",
    "            )\n",
    "            \n",
    "            bot_response = response.text.strip()\n",
    "            \n",
    "            # Save to conversation history\n",
    "            self.conversation_history.append((\"user\", user_message))\n",
    "            self.conversation_history.append((\"model\", bot_response))\n",
    "            \n",
    "            return bot_response\n",
    "        \n",
    "        except Exception as e:\n",
    "            error_msg = f\"I apologize, I'm having trouble processing your request right now. Error: {str(e)}\"\n",
    "            return error_msg\n",
    "    \n",
    "    def get_career_advice(self, current_situation, target_career):\n",
    "        \"\"\"\n",
    "        Get specific career advice\n",
    "        \n",
    "        Parameters:\n",
    "        current_situation: string describing current status\n",
    "        target_career: desired career path\n",
    "        \n",
    "        Returns:\n",
    "        advice string\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"The user is currently: {current_situation}\n",
    "\n",
    "They want to become: {target_career}\n",
    "\n",
    "Provide 3-5 specific, actionable pieces of advice for making this transition. \n",
    "Focus on:\n",
    "1. Most important skills to learn first\n",
    "2. Practical next steps they can take this week\n",
    "3. Common pitfalls to avoid\n",
    "4. Realistic timeline\n",
    "\n",
    "Keep the advice practical and encouraging.\"\"\"\n",
    "        \n",
    "        return self.chat(prompt, use_context=False)\n",
    "    \n",
    "    def explain_skill(self, skill_name):\n",
    "        \"\"\"\n",
    "        Explain what a skill is and why it's important\n",
    "        \n",
    "        Parameters:\n",
    "        skill_name: name of the skill\n",
    "        \n",
    "        Returns:\n",
    "        explanation string\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"Explain the skill \"{skill_name}\" in simple terms:\n",
    "1. What it is (1-2 sentences)\n",
    "2. Why it's valuable in the job market\n",
    "3. How long it typically takes to learn\n",
    "4. Best way to start learning it\n",
    "\n",
    "Keep it concise and practical.\"\"\"\n",
    "        \n",
    "        return self.chat(prompt, use_context=False)\n",
    "    \n",
    "    def roadmap_encouragement(self, completed_steps, total_steps):\n",
    "        \"\"\"\n",
    "        Provide encouragement based on progress\n",
    "        \n",
    "        Parameters:\n",
    "        completed_steps: number of completed steps\n",
    "        total_steps: total steps in roadmap\n",
    "        \n",
    "        Returns:\n",
    "        encouragement message\n",
    "        \"\"\"\n",
    "        progress_pct = (completed_steps / total_steps * 100) if total_steps > 0 else 0\n",
    "        \n",
    "        prompt = f\"\"\"The user has completed {completed_steps} out of {total_steps} steps in their learning roadmap ({progress_pct:.0f}% complete).\n",
    "\n",
    "Provide a short, encouraging message (2-3 sentences) that:\n",
    "1. Acknowledges their progress\n",
    "2. Motivates them to continue\n",
    "3. Reminds them of the value of consistency\n",
    "\n",
    "Be genuine and avoid clich√©s.\"\"\"\n",
    "        \n",
    "        return self.chat(prompt, use_context=True)\n",
    "    \n",
    "    def clear_history(self):\n",
    "        \"\"\"Clear conversation history\"\"\"\n",
    "        self.conversation_history.clear()\n",
    "    \n",
    "    def export_conversation(self):\n",
    "        \"\"\"Export conversation history as list\"\"\"\n",
    "        return list(self.conversation_history)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TESTING\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"PHASE 4: CAREER GUIDANCE CHATBOT - TEST\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Check API key\n",
    "    api_key = \"AIzaSyALQl3IlQPXT_dD8k5kvBA9j3aXenmfDAg\"\n",
    "    if not api_key:\n",
    "        print(\"\\n‚ö†Ô∏è GEMINI_API_KEY not found\")\n",
    "        print(\"Set it with: export GEMINI_API_KEY='your-key'\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Initialize chatbot\n",
    "    print(\"\\n[Step 1] Initializing chatbot...\")\n",
    "    chatbot = CareerGuidanceChatbot(api_key=api_key)\n",
    "    print(\"‚úì Chatbot initialized\")\n",
    "    \n",
    "    # Set user context\n",
    "    test_user = {\n",
    "        'name': 'Alex',\n",
    "        'target_career': 'Data Scientist',\n",
    "        'current_skills': ['Python', 'SQL', 'Excel', 'Statistics'],\n",
    "        'completed_steps': [0, 1, 2],  # 3 steps completed\n",
    "        'experience_level': 'beginner'\n",
    "    }\n",
    "    \n",
    "    chatbot.set_user_context(test_user)\n",
    "    print(f\"‚úì User context set for {test_user['name']}\")\n",
    "    \n",
    "    # Test 1: Career advice\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TEST 1: CAREER ADVICE\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\nUser: I'm a software developer wanting to transition to Data Science\")\n",
    "    response = chatbot.get_career_advice(\n",
    "        current_situation=\"Software Developer with 2 years experience in Python web development\",\n",
    "        target_career=\"Data Scientist\"\n",
    "    )\n",
    "    print(f\"\\nBot: {response}\")\n",
    "    \n",
    "    # Test 2: Skill explanation\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TEST 2: SKILL EXPLANATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\nUser: What is Machine Learning?\")\n",
    "    response = chatbot.explain_skill(\"Machine Learning\")\n",
    "    print(f\"\\nBot: {response}\")\n",
    "    \n",
    "    # Test 3: General question\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TEST 3: GENERAL QUESTION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\nUser: Should I get a Master's degree or learn through online courses?\")\n",
    "    response = chatbot.chat(\"Should I get a Master's degree or learn through online courses?\")\n",
    "    print(f\"\\nBot: {response}\")\n",
    "    \n",
    "    # Test 4: Follow-up question (tests conversation history)\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TEST 4: FOLLOW-UP QUESTION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\nUser: What about the cost difference?\")\n",
    "    response = chatbot.chat(\"What about the cost difference?\")\n",
    "    print(f\"\\nBot: {response}\")\n",
    "    \n",
    "    # Test 5: Encouragement\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TEST 5: PROGRESS ENCOURAGEMENT\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\nUser Progress: 3/8 steps completed\")\n",
    "    response = chatbot.roadmap_encouragement(completed_steps=3, total_steps=8)\n",
    "    print(f\"\\nBot: {response}\")\n",
    "    \n",
    "    # Test 6: Context-aware question\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TEST 6: CONTEXT-AWARE QUESTION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\nUser: What should I learn next based on my current skills?\")\n",
    "    response = chatbot.chat(\"What should I learn next based on my current skills?\")\n",
    "    print(f\"\\nBot: {response}\")\n",
    "    \n",
    "    # Export conversation\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CONVERSATION HISTORY\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    history = chatbot.export_conversation()\n",
    "    print(f\"\\nTotal messages: {len(history)}\")\n",
    "    print(\"\\nLast 3 exchanges:\")\n",
    "    for role, message in history[-6:]:\n",
    "        speaker = \"User\" if role == \"user\" else \"Bot\"\n",
    "        print(f\"\\n{speaker}: {message[:100]}...\" if len(message) > 100 else f\"\\n{speaker}: {message}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CHATBOT - TEST COMPLETE ‚úì\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\nKey Features:\")\n",
    "    print(\"  ‚úì Context-aware responses (user profile)\")\n",
    "    print(\"  ‚úì Conversation history (last 10 messages)\")\n",
    "    print(\"  ‚úì Career advice function\")\n",
    "    print(\"  ‚úì Skill explanation function\")\n",
    "    print(\"  ‚úì Progress encouragement\")\n",
    "    print(\"  ‚úì Natural follow-up questions\")\n",
    "    \n",
    "    print(\"\\nReady for:\")\n",
    "    print(\"  - Integration with FastAPI backend\")\n",
    "    print(\"  - Real-time chat interface\")\n",
    "    print(\"  - User-specific guidance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7668334,
     "sourceId": 12175770,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 279027754,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
